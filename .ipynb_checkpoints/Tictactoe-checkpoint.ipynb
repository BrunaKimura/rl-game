{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import Callback\n",
    "\n",
    "\n",
    "class TicTacToe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board_state = None\n",
    "\n",
    "    def set_state(self, new_state):\n",
    "        \"\"\" 2d array of cell positions of the board. 0 = cell not occupied,\n",
    "            1 = cross occupies cell, 2 = nought occupies cell.\n",
    "            Example: [\n",
    "                [0, 0, 1],\n",
    "                [0, 0, 2],\n",
    "                [0, 0, 0]\n",
    "            ] \"\"\"\n",
    "\n",
    "        new_state = np.array(new_state)\n",
    "\n",
    "        assert new_state.shape == (len(new_state), len(new_state))\n",
    "\n",
    "        self.board_state = new_state\n",
    "\n",
    "        return self.board_state\n",
    "\n",
    "    def is_finished(self):\n",
    "        \"\"\" 0 = not finished, 1 = cross win, 2 = nought win, 3 = tie \"\"\"\n",
    "\n",
    "        # Are we tied?\n",
    "        if self.board_state.flatten().tolist().count(0) == 0:\n",
    "            return 3\n",
    "\n",
    "        # Stolen: https://codereview.stackexchange.com/a/24775\n",
    "        positions_groups = (\n",
    "            [[(x, y) for y in range(self.get_board_size())] for x in range(self.get_board_size())] +  # horizontals\n",
    "            [[(x, y) for x in range(self.get_board_size())] for y in range(self.get_board_size())] +  # verticals\n",
    "            [[(d, d) for d in range(self.get_board_size())]] +  # diagonal from top-left to bottom-right\n",
    "            [[(2-d, d) for d in range(self.get_board_size())]]  # diagonal from top-right to bottom-left\n",
    "        )\n",
    "        for positions in positions_groups:\n",
    "            values = [self.board_state[x][y] for (x, y) in positions]\n",
    "            if len(set(values)) == 1 and values[0]:\n",
    "                return values[0]\n",
    "\n",
    "        # Game isn't finished\n",
    "        return 0\n",
    "\n",
    "    def get_board_size(self):\n",
    "        return len(self.board_state)\n",
    "\n",
    "    def get_turn(self):\n",
    "        \"\"\" Returns 1 for crosses turn, 2 for noughts turn \"\"\"\n",
    "\n",
    "        flattened_list = self.board_state.flatten().tolist()\n",
    "\n",
    "        if flattened_list.count(1) > flattened_list.count(2):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def make_move(self, x, y):\n",
    "        \"\"\" Updates the state with the requested new occupied cell \"\"\"\n",
    "        x = int(x)\n",
    "        y = int(y)\n",
    "        \n",
    "        # Sanity check\n",
    "        assert x < self.get_board_size() and y < self.get_board_size()\n",
    "        assert self.board_state[y][x] == 0\n",
    "        assert self.is_finished() == 0\n",
    "\n",
    "        new_state = self.board_state.copy()\n",
    "        new_state[y][x] = self.get_turn()\n",
    "\n",
    "        return self.set_state(new_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def translate_position_to_xy(position, board_size=3):\n",
    "        \"\"\" Takes a single number and maps it to x, y coordinates.\n",
    "            Example: 8 = 2, 2 for a board_size of 3 \"\"\"\n",
    "\n",
    "        x = position % board_size\n",
    "        y = position / board_size\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    action_space = Discrete(3**2)\n",
    "\n",
    "    def __init__(self, board_size=3, predict_for=None):\n",
    "        self.board_size = board_size\n",
    "        self.predict_for = predict_for\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=np.array([0 for cell in range(self.board_size ** 2)]),\n",
    "            high=np.array([2 for cell in range(self.board_size ** 2)])\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        if self.predict_for is not None:\n",
    "            self.tictactoe = TicTacToe()\n",
    "            self.tictactoe.set_state(self.predict_for)\n",
    "            return self.tictactoe.board_state.flatten()\n",
    "\n",
    "        self.tictactoe = TicTacToe()\n",
    "        self.tictactoe.set_state([\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "        move = self._get_random_move()\n",
    "\n",
    "        self.tictactoe.make_move(move[0], move[1])\n",
    "\n",
    "        return self.tictactoe.board_state.flatten()\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.predict_for is not None:\n",
    "            return self.tictactoe.board_state.flatten(), 0, True, {}\n",
    "\n",
    "        translated_action = TicTacToe.translate_position_to_xy(action)\n",
    "\n",
    "        try:\n",
    "            self.tictactoe.make_move(translated_action[0], translated_action[1])\n",
    "\n",
    "        except AssertionError:\n",
    "            return self.tictactoe.board_state.flatten(), -1, True, {}\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        winner = self.tictactoe.is_finished()\n",
    "        if winner == 0:\n",
    "            move = self._get_random_move()\n",
    "            self.tictactoe.make_move(move[0], move[1])\n",
    "\n",
    "            next_winner = self.tictactoe.is_finished()\n",
    "            if next_winner == 1:\n",
    "                reward = -1\n",
    "                done = True\n",
    "            elif next_winner == 3:\n",
    "                reward = 0\n",
    "                done = True\n",
    "\n",
    "        elif winner == 2:\n",
    "            reward = 1\n",
    "            done = True\n",
    "\n",
    "        elif winner == 3:\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        return self.tictactoe.board_state.flatten(), reward, done, {}\n",
    "\n",
    "    def _get_random_move(self):\n",
    "        assert self.tictactoe.is_finished() == 0\n",
    "\n",
    "        positions = []\n",
    "        for x in range(self.board_size):\n",
    "            for y in range(self.board_size):\n",
    "                if self.tictactoe.board_state[y][x] == 0:\n",
    "                    positions.append((x, y))\n",
    "\n",
    "        return positions[np.random.choice(len(positions), 1)[0]]\n",
    "\n",
    "    \n",
    "class ModelIntervalCheckpoint(Callback):\n",
    "    def __init__(self, interval, verbose=0):\n",
    "        super(ModelIntervalCheckpoint, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.step = 0\n",
    "\n",
    "        self.rewards = []\n",
    "        self.last_max = -1\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "\n",
    "    def on_step_begin(self, step, logs):\n",
    "        if self.step % self.interval == 0:\n",
    "            if len(self.rewards) > 0:\n",
    "                mean_reward = np.nanmean(self.rewards, axis=0)\n",
    "                if mean_reward > self.last_max:\n",
    "                    filename = 'saved-weights/%s.h5f' % mean_reward\n",
    "                    print(\"\\nSaving model checkpoint with mean reward %s to %s\" % (mean_reward, filename))\n",
    "\n",
    "                    self.model.save_weights(filename, overwrite=True)\n",
    "\n",
    "                    self.last_max = mean_reward\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "\n",
    "        self.rewards.append(logs['reward'])\n",
    "        self.step += 1\n",
    "        \n",
    "def predict(board_state, model_path):\n",
    "    env = TicTacToeEnv(predict_for=board_state)\n",
    "\n",
    "    dqn = build_dqn(env)\n",
    "\n",
    "    dqn.load_weights(model_path)\n",
    "\n",
    "    dqn.test(env, nb_episodes=1, visualize=False, verbose=0)\n",
    "\n",
    "    return dqn.recent_action\n",
    "\n",
    "\n",
    "def build_dqn(env):\n",
    "    nb_actions = env.action_space.n\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "    memory = SequentialMemory(limit=5000000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    log_interval = 10000\n",
    "\n",
    "    dqn = DQNAgent(\n",
    "        model=model,\n",
    "        nb_actions=nb_actions,\n",
    "        memory=memory,\n",
    "        nb_steps_warmup=1000,\n",
    "        enable_dueling_network=False,\n",
    "        target_model_update=1e-2,\n",
    "        policy=policy\n",
    "    )\n",
    "\n",
    "    dqn.compile(Adam(lr=1e-5), metrics=['accuracy', 'mae'])\n",
    "\n",
    "    return dqn\n",
    "\n",
    "\n",
    "env = TicTacToeEnv()\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "memory = SequentialMemory(limit=5000000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "log_interval = 10000\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    memory=memory,\n",
    "    nb_steps_warmup=1000,\n",
    "    enable_dueling_network=False,\n",
    "    target_model_update=1e-2,\n",
    "    policy=policy\n",
    ")\n",
    "\n",
    "dqn.compile(Adam(lr=1e-5), metrics=['accuracy', 'mae'])\n",
    "\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1,\n",
    "    callbacks=[ModelIntervalCheckpoint(interval=log_interval)],\n",
    "    log_interval=log_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_state = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "predict(board_state, 'saved-weights/-0.2235.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
