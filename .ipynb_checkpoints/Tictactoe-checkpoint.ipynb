{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import Callback\n",
    "from spinup import ppo\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning com Jogo da Velha e TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Por Bruna Kimura, Elisa Malzoni e Raphael Costa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa aula tem como objetivo discorrer e mostrar dois métodos de Reinforcement Learning: o Proximal Policy Optimization (PPO) e o Deep Q-Learning.\n",
    "\n",
    "Para tal, utilizaremos um ambiente simulado de um jogo da velha. No jogo da velha, dois jogadores duelam e, quem conseguir 3 Círculos (jogador 1) ou 3 X's (Jogador 2) em sequência em uma linha, coluna ou diagonal vence. Nosso objetivo nesta aula é construir um Bot que aprenda a jogar o jogo da velha, e, mais do que isso, ganhe o máximo possível, permitindo-se apenas empatar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diagrama](img/rl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para um experimento de Reinforcement Learning, precisamos definir 4 conceitos principais: Agente, Estado, Ambiente e Recompensa.\n",
    "\n",
    "O agente é aquele que realiza ações. No nosso caso, queremos programar um agente capaz de jogar o Jogo da Velha (e, é claro, vencer). Este Agente é treinado dentro de um ambiente, na qual o Agente se baseia para tomar suas decisões. No nosso caso, este ambiente será uma matriz de 9 posições, simulando o tabuleiro de jogo da velha. Cada passo que o Agente da dentro do ambiente chamamos de Ação, ou seja, são todas as possibilidades de ação que o Agente tem. Cada Ação leva o Agente para um novo Estado, que corresponde a uma situação concreta e imediata da onde o Agente se encontra. Ainda, cada vez que o Agente faz uma Ação e se encontra em um novo Estado, definimos uma Recompensa dada por aquela Ação. No nosso jogo, a maior recompensa que o Agente terá é 1, dada quando ele consegue uma vitória. Quando perder, sua recompensa será de -1, e, por último, a recompensa por um empate é 0 (Nula)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tabuleiro](img/tabuleiro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir o ambiente, utilizaremos duas classes: TicTacToe e TicTacToeEnv.\n",
    "\n",
    "Na <code>TicTacToe</code> definiremos o que é o jogo da velha, é a classe que descreve as regras do jogo em si. Assim, ela simulará uma matriz de 9 posições, que podem assumir 3 diferentes valores: 0, 1 e 2, onde 0 é uma posição vazia, 1 é o jogador círculo e 2 é o jogador X. Ainda, é ela quem confere se há um ganhador após determinada jogada.\n",
    "\n",
    "Ja a <code>TicTacToeEnv</code> é a classe responsável por efetivamente ser o ambiente de Reinforcement Learning do nosso bot. Ela baseia-se na classe TicTacToe para efetuar movimentos, treinar o bot, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para codificarmos um ambiente de Reinforcement Learning precisamos basicamente de duas funções: reset e step.\n",
    "\n",
    "A função <code>reset</code>, como o próprio nome diz, tem o papel de iniciar o ambiente a cada iteração. Assim, ela iniciará a matriz com todas as 9 posições zeradas e efetuará a primeira jogada do jogador 1. As jogadas do jogador 1, aquele contra o qual nosso bot está jogando, serão efetuadas aleatoriamente.\n",
    "\n",
    "A função <code>step</code> é responsável por indicar, no ambiente, qual foi a jogada que o nosso bot realizou. Ainda, ela também é responsável por dizer se o jogo acabou. Portanto, seu retorno é o estado do tabuleiro após a jogada do bot, a recompensa do agente para aquela ação tomada e se o jogo terminou.  \n",
    "\n",
    "Para este projeto, o design das funções do TicTacToeEnv foi baseado nos enviroments especificados na biblioteca do Keras, já que esta possui a implementação do algoritmo de Deep Q-Learning e, a biblioteca que possui a implementação do PPO, a OpenAI Spinning Up, é baseada na biblioteca do Keras para simular seus ambientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board_state = None\n",
    "    \n",
    "    def set_state(self, new_state):\n",
    "        \"\"\" 2d array of cell positions of the board. 0 = cell not occupied,\n",
    "            1 = cross occupies cell, 2 = nought occupies cell.\n",
    "            Example: [\n",
    "                [0, 0, 1],\n",
    "                [0, 0, 2],\n",
    "                [0, 0, 0]\n",
    "            ] \"\"\"\n",
    "\n",
    "        new_state = np.array(new_state)\n",
    "\n",
    "        assert new_state.shape == (len(new_state), len(new_state))\n",
    "\n",
    "        self.board_state = new_state\n",
    "\n",
    "        return self.board_state\n",
    "\n",
    "    def is_finished(self):\n",
    "        \"\"\" 0 = not finished, 1 = cross win, 2 = nought win, 3 = tie \"\"\"\n",
    "\n",
    "        # Are we tied?\n",
    "        if self.board_state.flatten().tolist().count(0) == 0:\n",
    "            return 3\n",
    "\n",
    "        # Stolen: https://codereview.stackexchange.com/a/24775\n",
    "        positions_groups = (\n",
    "            [[(x, y) for y in range(self.get_board_size())] for x in range(self.get_board_size())] +  # horizontals\n",
    "            [[(x, y) for x in range(self.get_board_size())] for y in range(self.get_board_size())] +  # verticals\n",
    "            [[(d, d) for d in range(self.get_board_size())]] +  # diagonal from top-left to bottom-right\n",
    "            [[(2-d, d) for d in range(self.get_board_size())]]  # diagonal from top-right to bottom-left\n",
    "        )\n",
    "        for positions in positions_groups:\n",
    "            values = [self.board_state[x][y] for (x, y) in positions]\n",
    "            if len(set(values)) == 1 and values[0]:\n",
    "                return values[0]\n",
    "\n",
    "        # Game isn't finished\n",
    "        return 0\n",
    "\n",
    "    def get_board_size(self):\n",
    "        return len(self.board_state)\n",
    "\n",
    "    def get_turn(self):\n",
    "        \"\"\" Returns 1 for crosses turn, 2 for noughts turn \"\"\"\n",
    "\n",
    "        flattened_list = self.board_state.flatten().tolist()\n",
    "\n",
    "        if flattened_list.count(1) > flattened_list.count(2):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def make_move(self, x, y):\n",
    "        \"\"\" Updates the state with the requested new occupied cell \"\"\"\n",
    "        x = int(x)\n",
    "        y = int(y)\n",
    "        \n",
    "        # Sanity check\n",
    "        assert x < self.get_board_size() and y < self.get_board_size()\n",
    "        assert self.board_state[y][x] == 0\n",
    "        assert self.is_finished() == 0\n",
    "\n",
    "        new_state = self.board_state.copy()\n",
    "        new_state[y][x] = self.get_turn()\n",
    "\n",
    "        return self.set_state(new_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def translate_position_to_xy(position, board_size=3):\n",
    "        \"\"\" Takes a single number and maps it to x, y coordinates.\n",
    "            Example: 8 = 2, 2 for a board_size of 3 \"\"\"\n",
    "\n",
    "        x = position % board_size\n",
    "        y = position / board_size\n",
    "\n",
    "        return x, y### TicTacToe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TicTacToeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TicTacToeclass TicTacToeEnv:\n",
    "class TicTacToeEnv:\n",
    "    action_space = Discrete(3**2)\n",
    "\n",
    "    def __init__(self, board_size=3, predict_for=None):\n",
    "        self.board_size = board_size\n",
    "        self.predict_for = predict_for\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=np.array([0 for cell in range(self.board_size ** 2)]),\n",
    "            high=np.array([2 for cell in range(self.board_size ** 2)])\n",
    "        )\n",
    "        self.tictactoe = None\n",
    "        \n",
    "    def render(self):\n",
    "        if self.tictactoe is not None and self.tictactoe.board_state is not None:\n",
    "            print(self.tictactoe.board_state)\n",
    "                    \n",
    "    def reset(self):\n",
    "        if self.predict_for is not None:\n",
    "            self.tictactoe = TicTacToe()\n",
    "            self.tictactoe.set_state(self.predict_for)\n",
    "            return self.tictactoe.board_state.flatten()\n",
    "\n",
    "        self.tictactoe = TicTacToe()\n",
    "        self.tictactoe.set_state([\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "        move = self._get_random_move()\n",
    "\n",
    "        self.tictactoe.make_move(move[0], move[1])\n",
    "\n",
    "        return self.tictactoe.board_state.flatten()\n",
    "    \n",
    "    def resetX1(self):\n",
    "        self.tictactoe = TicTacToe()\n",
    "        self.tictactoe.set_state([\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "\n",
    "        return self.tictactoe.board_state.flatten()\n",
    "    \n",
    "    def stepX1(self, action):\n",
    "        translated_action = TicTacToe.translate_position_to_xy(action)\n",
    "        \n",
    "        try:\n",
    "            self.tictactoe.make_move(translated_action[0], translated_action[1])\n",
    "\n",
    "        except AssertionError:\n",
    "            return self.tictactoe.board_state.flatten(), -1, True, {}\n",
    "        \n",
    "        winner = self.tictactoe.is_finished()\n",
    "        #winner == 0: nao acabou\n",
    "        #winner == 1: ganhou o 1\n",
    "        #winner == 2: ganhou o 2\n",
    "        #winner == 3: empate\n",
    "        return self.tictactoe.board_state.flatten(), winner \n",
    "    \n",
    "        \n",
    "\n",
    "    def step(self, action):\n",
    "        if self.predict_for is not None:\n",
    "            return self.tictactoe.board_state.flatten(), 0, True, {}\n",
    "\n",
    "        translated_action = TicTacToe.translate_position_to_xy(action)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            self.tictactoe.make_move(translated_action[0], translated_action[1])\n",
    "\n",
    "        except AssertionError:\n",
    "            return self.tictactoe.board_state.flatten(), -1, True, {}\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        winner = self.tictactoe.is_finished()\n",
    "        if winner == 0:\n",
    "            move = self._get_random_move()\n",
    "            self.tictactoe.make_move(move[0], move[1])\n",
    "\n",
    "            next_winner = self.tictactoe.is_finished()\n",
    "            if next_winner == 1:\n",
    "                reward = -1\n",
    "                done = True\n",
    "            elif next_winner == 3:\n",
    "                reward = 0\n",
    "                done = True\n",
    "\n",
    "        elif winner == 2:\n",
    "            reward = 1\n",
    "            done = True\n",
    "\n",
    "        elif winner == 3:\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        return self.tictactoe.board_state.flatten(), reward, done, {}\n",
    "\n",
    "    def _get_random_move(self):\n",
    "        assert self.tictactoe.is_finished() == 0\n",
    "\n",
    "        positions = []\n",
    "        for x in range(self.board_size):\n",
    "            for y in range(self.board_size):\n",
    "                if self.tictactoe.board_state[y][x] == 0:\n",
    "                    positions.append((x, y))\n",
    "\n",
    "        return positions[np.random.choice(len(positions), 1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TicTacToe com o PPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o Proximal Policy Optimization, como dito anteriormente, utilizaremos a implementação do OpenAI Spinning Up. Inicialmente, definimos uma váriavel que aponta para o ambiente que queremos utilizar, a <code>env_fn</code>.\n",
    "\n",
    "Ainda, a biblioteca define um Logger, a váriavel <code>logger_kwargs</code> que é a classe que printa o andamento do treinamento (como pode ser visto ao rodar as células seguintes) e salva os pesos e histórico do experimento em arquivos. Neste caso, salvaremos tais dados na pasta que este Notebook se encontra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, chamamos efetivamente a função <code>ppo</code>, passando, basicamente, 5 argumentos obrigatórios:\n",
    "- <code>env_fn</code>: váriavel do ambiente\n",
    "- <code>ac_kwargs</code>: váriavel que define a rede neural que será utilizada no experimento.\n",
    "- <code>steps_per_epoch</code>: Quantos passos por epóca o ambiente pode dar\n",
    "- <code>epochs</code>: número de épocas que serão realizadas no treinamento\n",
    "- <code>logger_kwargs</code>: váriavel do logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fn = lambda : TicTacToeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atenção!!! O treinamento demora, não execute-a se você não deseja sobrepor o treinamento que ja fizemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1mLogging data to spinupPpo/progress.txt\u001b[0m\n",
      "\u001b[36;1mSaving config:\n",
      "\u001b[0m\n",
      "{\n",
      "    \"ac_kwargs\":\t{\n",
      "        \"activation\":\t\"relu\",\n",
      "        \"hidden_sizes\":\t[\n",
      "            64,\n",
      "            64\n",
      "        ]\n",
      "    },\n",
      "    \"actor_critic\":\t\"mlp_actor_critic\",\n",
      "    \"clip_ratio\":\t0.2,\n",
      "    \"env_fn\":\t\"<function <lambda> at 0x1282128c8>\",\n",
      "    \"epochs\":\t50,\n",
      "    \"exp_name\":\t\"experiment\",\n",
      "    \"gamma\":\t0.99,\n",
      "    \"lam\":\t0.97,\n",
      "    \"logger\":\t{\n",
      "        \"<spinup.utils.logx.EpochLogger object at 0x1282650f0>\":\t{\n",
      "            \"epoch_dict\":\t{},\n",
      "            \"exp_name\":\t\"experiment\",\n",
      "            \"first_row\":\ttrue,\n",
      "            \"log_current_row\":\t{},\n",
      "            \"log_headers\":\t[],\n",
      "            \"output_dir\":\t\"spinupPpo\",\n",
      "            \"output_file\":\t{\n",
      "                \"<_io.TextIOWrapper name='spinupPpo/progress.txt' mode='w' encoding='UTF-8'>\":\t{\n",
      "                    \"mode\":\t\"w\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"logger_kwargs\":\t{\n",
      "        \"exp_name\":\t\"experiment\",\n",
      "        \"output_dir\":\t\"spinupPpo\"\n",
      "    },\n",
      "    \"max_ep_len\":\t1000,\n",
      "    \"pi_lr\":\t0.0003,\n",
      "    \"save_freq\":\t10,\n",
      "    \"seed\":\t0,\n",
      "    \"steps_per_epoch\":\t5000,\n",
      "    \"target_kl\":\t0.01,\n",
      "    \"train_pi_iters\":\t80,\n",
      "    \"train_v_iters\":\t80,\n",
      "    \"vf_lr\":\t0.001\n",
      "}\n",
      "WARNING:tensorflow:From /Users/raphacosta/Desktop/Insper7/machineLearning/projeto/spinningup/spinup/algos/ppo/core.py:31: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/raphacosta/Desktop/Insper7/machineLearning/projeto/spinningup/spinup/algos/ppo/core.py:71: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "\u001b[32;1m\n",
      "Number of parameters: \t pi: 5385, \t v: 4865\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/raphacosta/Desktop/Insper7/machineLearning/projeto/spinningup/spinup/utils/mpi_tf.py:63: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "WARNING:tensorflow:From /Users/raphacosta/Desktop/Insper7/machineLearning/projeto/spinningup/spinup/utils/logx.py:226: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: spinupPpo/simple_save/saved_model.pb\n",
      "\u001b[32;1mEarly stopping at step 77 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               0 |\n",
      "|      AverageEpRet |          -0.917 |\n",
      "|          StdEpRet |           0.375 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            2.56 |\n",
      "|      AverageVVals |           0.168 |\n",
      "|          StdVVals |           0.217 |\n",
      "|          MaxVVals |            1.07 |\n",
      "|          MinVVals |          -0.314 |\n",
      "| TotalEnvInteracts |           5e+03 |\n",
      "|            LossPi |        1.43e-07 |\n",
      "|             LossV |            1.29 |\n",
      "|       DeltaLossPi |         -0.0395 |\n",
      "|        DeltaLossV |            -1.1 |\n",
      "|           Entropy |            2.12 |\n",
      "|                KL |          0.0152 |\n",
      "|          ClipFrac |           0.181 |\n",
      "|          StopIter |              77 |\n",
      "|              Time |            4.43 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "\u001b[32;1mEarly stopping at step 79 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               1 |\n",
      "|      AverageEpRet |          -0.884 |\n",
      "|          StdEpRet |           0.435 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            2.64 |\n",
      "|      AverageVVals |           -0.85 |\n",
      "|          StdVVals |           0.134 |\n",
      "|          MaxVVals |          -0.315 |\n",
      "|          MinVVals |           -1.25 |\n",
      "| TotalEnvInteracts |           1e+04 |\n",
      "|            LossPi |        6.87e-09 |\n",
      "|             LossV |           0.241 |\n",
      "|       DeltaLossPi |         -0.0444 |\n",
      "|        DeltaLossV |         -0.0169 |\n",
      "|           Entropy |            2.14 |\n",
      "|                KL |          0.0151 |\n",
      "|          ClipFrac |           0.241 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            8.51 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "\u001b[32;1mEarly stopping at step 33 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               2 |\n",
      "|      AverageEpRet |           -0.84 |\n",
      "|          StdEpRet |           0.502 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            2.75 |\n",
      "|      AverageVVals |          -0.822 |\n",
      "|          StdVVals |           0.107 |\n",
      "|          MaxVVals |          -0.201 |\n",
      "|          MinVVals |           -1.12 |\n",
      "| TotalEnvInteracts |         1.5e+04 |\n",
      "|            LossPi |        7.25e-09 |\n",
      "|             LossV |           0.298 |\n",
      "|       DeltaLossPi |         -0.0396 |\n",
      "|        DeltaLossV |          -0.016 |\n",
      "|           Entropy |            2.11 |\n",
      "|                KL |          0.0161 |\n",
      "|          ClipFrac |           0.196 |\n",
      "|          StopIter |              33 |\n",
      "|              Time |            12.1 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "\u001b[32;1mEarly stopping at step 30 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               3 |\n",
      "|      AverageEpRet |          -0.772 |\n",
      "|          StdEpRet |           0.573 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            2.89 |\n",
      "|      AverageVVals |          -0.768 |\n",
      "|          StdVVals |           0.128 |\n",
      "|          MaxVVals |           -0.12 |\n",
      "|          MinVVals |           -1.09 |\n",
      "| TotalEnvInteracts |           2e+04 |\n",
      "|            LossPi |       -4.58e-09 |\n",
      "|             LossV |           0.368 |\n",
      "|       DeltaLossPi |         -0.0426 |\n",
      "|        DeltaLossV |         -0.0203 |\n",
      "|           Entropy |            2.08 |\n",
      "|                KL |          0.0155 |\n",
      "|          ClipFrac |           0.215 |\n",
      "|          StopIter |              30 |\n",
      "|              Time |            15.9 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "\u001b[32;1mEarly stopping at step 40 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               4 |\n",
      "|      AverageEpRet |          -0.668 |\n",
      "|          StdEpRet |            0.68 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            2.97 |\n",
      "|      AverageVVals |          -0.691 |\n",
      "|          StdVVals |           0.143 |\n",
      "|          MaxVVals |            0.06 |\n",
      "|          MinVVals |           -1.02 |\n",
      "| TotalEnvInteracts |         2.5e+04 |\n",
      "|            LossPi |       -2.16e-08 |\n",
      "|             LossV |           0.494 |\n",
      "|       DeltaLossPi |         -0.0509 |\n",
      "|        DeltaLossV |         -0.0331 |\n",
      "|           Entropy |            2.01 |\n",
      "|                KL |          0.0158 |\n",
      "|          ClipFrac |           0.239 |\n",
      "|          StopIter |              40 |\n",
      "|              Time |            19.3 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 3 steps.\n",
      "\u001b[32;1mEarly stopping at step 33 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               5 |\n",
      "|      AverageEpRet |          -0.533 |\n",
      "|          StdEpRet |           0.781 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.04 |\n",
      "|      AverageVVals |          -0.576 |\n",
      "|          StdVVals |           0.172 |\n",
      "|          MaxVVals |           0.385 |\n",
      "|          MinVVals |           -1.09 |\n",
      "| TotalEnvInteracts |           3e+04 |\n",
      "|            LossPi |       -1.53e-09 |\n",
      "|             LossV |           0.627 |\n",
      "|       DeltaLossPi |         -0.0479 |\n",
      "|        DeltaLossV |         -0.0461 |\n",
      "|           Entropy |            1.94 |\n",
      "|                KL |          0.0157 |\n",
      "|          ClipFrac |           0.194 |\n",
      "|          StopIter |              33 |\n",
      "|              Time |            22.6 |\n",
      "---------------------------------------\n",
      "\u001b[32;1mEarly stopping at step 26 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               6 |\n",
      "|      AverageEpRet |          -0.435 |\n",
      "|          StdEpRet |           0.812 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.13 |\n",
      "|      AverageVVals |          -0.434 |\n",
      "|          StdVVals |           0.206 |\n",
      "|          MaxVVals |           0.592 |\n",
      "|          MinVVals |          -0.869 |\n",
      "| TotalEnvInteracts |         3.5e+04 |\n",
      "|            LossPi |       -2.19e-09 |\n",
      "|             LossV |           0.631 |\n",
      "|       DeltaLossPi |         -0.0429 |\n",
      "|        DeltaLossV |         -0.0417 |\n",
      "|           Entropy |            1.86 |\n",
      "|                KL |          0.0158 |\n",
      "|          ClipFrac |            0.19 |\n",
      "|          StopIter |              26 |\n",
      "|              Time |            25.9 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "\u001b[32;1mEarly stopping at step 34 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               7 |\n",
      "|      AverageEpRet |           -0.32 |\n",
      "|          StdEpRet |           0.859 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.25 |\n",
      "|      AverageVVals |          -0.328 |\n",
      "|          StdVVals |           0.244 |\n",
      "|          MaxVVals |           0.688 |\n",
      "|          MinVVals |          -0.958 |\n",
      "| TotalEnvInteracts |           4e+04 |\n",
      "|            LossPi |        1.83e-08 |\n",
      "|             LossV |           0.702 |\n",
      "|       DeltaLossPi |         -0.0398 |\n",
      "|        DeltaLossV |         -0.0537 |\n",
      "|           Entropy |            1.77 |\n",
      "|                KL |          0.0156 |\n",
      "|          ClipFrac |           0.177 |\n",
      "|          StopIter |              34 |\n",
      "|              Time |            29.2 |\n",
      "---------------------------------------\n",
      "\u001b[32;1mEarly stopping at step 7 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               8 |\n",
      "|      AverageEpRet |          -0.158 |\n",
      "|          StdEpRet |           0.888 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.27 |\n",
      "|      AverageVVals |          -0.221 |\n",
      "|          StdVVals |           0.223 |\n",
      "|          MaxVVals |           0.808 |\n",
      "|          MinVVals |          -0.813 |\n",
      "| TotalEnvInteracts |         4.5e+04 |\n",
      "|            LossPi |       -1.43e-08 |\n",
      "|             LossV |           0.718 |\n",
      "|       DeltaLossPi |         -0.0145 |\n",
      "|        DeltaLossV |         -0.0608 |\n",
      "|           Entropy |            1.69 |\n",
      "|                KL |          0.0186 |\n",
      "|          ClipFrac |           0.189 |\n",
      "|          StopIter |               7 |\n",
      "|              Time |            32.3 |\n",
      "---------------------------------------\n",
      "\u001b[32;1mEarly stopping at step 53 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               9 |\n",
      "|      AverageEpRet |          -0.152 |\n",
      "|          StdEpRet |           0.895 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.26 |\n",
      "|      AverageVVals |         -0.0553 |\n",
      "|          StdVVals |           0.249 |\n",
      "|          MaxVVals |           0.911 |\n",
      "|          MinVVals |          -0.624 |\n",
      "| TotalEnvInteracts |           5e+04 |\n",
      "|            LossPi |       -6.68e-09 |\n",
      "|             LossV |           0.688 |\n",
      "|       DeltaLossPi |         -0.0397 |\n",
      "|        DeltaLossV |         -0.0475 |\n",
      "|           Entropy |            1.64 |\n",
      "|                KL |          0.0153 |\n",
      "|          ClipFrac |           0.182 |\n",
      "|          StopIter |              53 |\n",
      "|              Time |            35.8 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 3 steps.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: spinupPpo/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |              10 |\n",
      "|      AverageEpRet |         -0.0194 |\n",
      "|          StdEpRet |           0.907 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.35 |\n",
      "|      AverageVVals |         -0.0357 |\n",
      "|          StdVVals |           0.298 |\n",
      "|          MaxVVals |             1.1 |\n",
      "|          MinVVals |          -0.839 |\n",
      "| TotalEnvInteracts |         5.5e+04 |\n",
      "|            LossPi |        3.05e-09 |\n",
      "|             LossV |           0.687 |\n",
      "|       DeltaLossPi |         -0.0365 |\n",
      "|        DeltaLossV |         -0.0574 |\n",
      "|           Entropy |            1.53 |\n",
      "|                KL |          0.0132 |\n",
      "|          ClipFrac |           0.161 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            39.7 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "\u001b[32;1mEarly stopping at step 52 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |              11 |\n",
      "|      AverageEpRet |          0.0274 |\n",
      "|          StdEpRet |           0.906 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.34 |\n",
      "|      AverageVVals |          0.0565 |\n",
      "|          StdVVals |           0.317 |\n",
      "|          MaxVVals |            1.11 |\n",
      "|          MinVVals |          -0.788 |\n",
      "| TotalEnvInteracts |           6e+04 |\n",
      "|            LossPi |       -4.58e-09 |\n",
      "|             LossV |            0.67 |\n",
      "|       DeltaLossPi |         -0.0331 |\n",
      "|        DeltaLossV |         -0.0527 |\n",
      "|           Entropy |            1.41 |\n",
      "|                KL |          0.0152 |\n",
      "|          ClipFrac |           0.152 |\n",
      "|          StopIter |              52 |\n",
      "|              Time |            43.2 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              12 |\n",
      "|      AverageEpRet |           0.141 |\n",
      "|          StdEpRet |           0.893 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.38 |\n",
      "|      AverageVVals |           0.139 |\n",
      "|          StdVVals |           0.315 |\n",
      "|          MaxVVals |            1.19 |\n",
      "|          MinVVals |          -0.729 |\n",
      "| TotalEnvInteracts |         6.5e+04 |\n",
      "|            LossPi |        8.77e-09 |\n",
      "|             LossV |           0.632 |\n",
      "|       DeltaLossPi |         -0.0352 |\n",
      "|        DeltaLossV |          -0.058 |\n",
      "|           Entropy |            1.32 |\n",
      "|                KL |          0.0106 |\n",
      "|          ClipFrac |           0.177 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |              47 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              13 |\n",
      "|      AverageEpRet |           0.188 |\n",
      "|          StdEpRet |           0.886 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.37 |\n",
      "|      AverageVVals |           0.218 |\n",
      "|          StdVVals |           0.317 |\n",
      "|          MaxVVals |            1.27 |\n",
      "|          MinVVals |          -0.887 |\n",
      "| TotalEnvInteracts |           7e+04 |\n",
      "|            LossPi |       -5.34e-09 |\n",
      "|             LossV |           0.597 |\n",
      "|       DeltaLossPi |         -0.0318 |\n",
      "|        DeltaLossV |         -0.0603 |\n",
      "|           Entropy |            1.25 |\n",
      "|                KL |         0.00981 |\n",
      "|          ClipFrac |           0.138 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            50.7 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              14 |\n",
      "|      AverageEpRet |           0.291 |\n",
      "|          StdEpRet |           0.872 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.37 |\n",
      "|      AverageVVals |           0.284 |\n",
      "|          StdVVals |           0.356 |\n",
      "|          MaxVVals |            1.36 |\n",
      "|          MinVVals |          -0.662 |\n",
      "| TotalEnvInteracts |         7.5e+04 |\n",
      "|            LossPi |        5.34e-09 |\n",
      "|             LossV |            0.58 |\n",
      "|       DeltaLossPi |         -0.0303 |\n",
      "|        DeltaLossV |         -0.0634 |\n",
      "|           Entropy |            1.16 |\n",
      "|                KL |          0.0095 |\n",
      "|          ClipFrac |           0.107 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            54.6 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              15 |\n",
      "|      AverageEpRet |            0.29 |\n",
      "|          StdEpRet |           0.872 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.38 |\n",
      "|      AverageVVals |           0.349 |\n",
      "|          StdVVals |            0.34 |\n",
      "|          MaxVVals |            1.33 |\n",
      "|          MinVVals |          -0.899 |\n",
      "| TotalEnvInteracts |           8e+04 |\n",
      "|            LossPi |        7.63e-09 |\n",
      "|             LossV |           0.565 |\n",
      "|       DeltaLossPi |          -0.028 |\n",
      "|        DeltaLossV |         -0.0545 |\n",
      "|           Entropy |             1.1 |\n",
      "|                KL |          0.0117 |\n",
      "|          ClipFrac |           0.109 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            58.3 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              16 |\n",
      "|      AverageEpRet |            0.31 |\n",
      "|          StdEpRet |           0.864 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.39 |\n",
      "|      AverageVVals |           0.344 |\n",
      "|          StdVVals |           0.344 |\n",
      "|          MaxVVals |            1.29 |\n",
      "|          MinVVals |          -0.757 |\n",
      "| TotalEnvInteracts |         8.5e+04 |\n",
      "|            LossPi |        5.34e-09 |\n",
      "|             LossV |           0.526 |\n",
      "|       DeltaLossPi |         -0.0326 |\n",
      "|        DeltaLossV |         -0.0535 |\n",
      "|           Entropy |            1.07 |\n",
      "|                KL |          0.0127 |\n",
      "|          ClipFrac |           0.136 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            63.1 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              17 |\n",
      "|      AverageEpRet |           0.359 |\n",
      "|          StdEpRet |           0.842 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |             3.4 |\n",
      "|      AverageVVals |           0.391 |\n",
      "|          StdVVals |           0.389 |\n",
      "|          MaxVVals |            1.41 |\n",
      "|          MinVVals |           -1.04 |\n",
      "| TotalEnvInteracts |           9e+04 |\n",
      "|            LossPi |        1.34e-08 |\n",
      "|             LossV |            0.49 |\n",
      "|       DeltaLossPi |         -0.0289 |\n",
      "|        DeltaLossV |         -0.0545 |\n",
      "|           Entropy |           0.962 |\n",
      "|                KL |         0.00977 |\n",
      "|          ClipFrac |           0.129 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |              68 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              18 |\n",
      "|      AverageEpRet |           0.384 |\n",
      "|          StdEpRet |           0.837 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.38 |\n",
      "|      AverageVVals |           0.425 |\n",
      "|          StdVVals |           0.382 |\n",
      "|          MaxVVals |            1.34 |\n",
      "|          MinVVals |          -0.976 |\n",
      "| TotalEnvInteracts |         9.5e+04 |\n",
      "|            LossPi |       -2.48e-09 |\n",
      "|             LossV |           0.471 |\n",
      "|       DeltaLossPi |         -0.0293 |\n",
      "|        DeltaLossV |         -0.0514 |\n",
      "|           Entropy |           0.913 |\n",
      "|                KL |          0.0102 |\n",
      "|          ClipFrac |           0.113 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            71.9 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              19 |\n",
      "|      AverageEpRet |           0.407 |\n",
      "|          StdEpRet |           0.828 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.33 |\n",
      "|      AverageVVals |           0.428 |\n",
      "|          StdVVals |           0.397 |\n",
      "|          MaxVVals |            1.25 |\n",
      "|          MinVVals |          -0.967 |\n",
      "| TotalEnvInteracts |           1e+05 |\n",
      "|            LossPi |       -1.03e-08 |\n",
      "|             LossV |           0.463 |\n",
      "|       DeltaLossPi |         -0.0255 |\n",
      "|        DeltaLossV |         -0.0444 |\n",
      "|           Entropy |           0.865 |\n",
      "|                KL |          0.0109 |\n",
      "|          ClipFrac |           0.117 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            76.6 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: spinupPpo/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |              20 |\n",
      "|      AverageEpRet |           0.477 |\n",
      "|          StdEpRet |           0.797 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.35 |\n",
      "|      AverageVVals |           0.471 |\n",
      "|          StdVVals |           0.379 |\n",
      "|          MaxVVals |            1.35 |\n",
      "|          MinVVals |          -0.825 |\n",
      "| TotalEnvInteracts |        1.05e+05 |\n",
      "|            LossPi |       -2.29e-09 |\n",
      "|             LossV |           0.444 |\n",
      "|       DeltaLossPi |         -0.0236 |\n",
      "|        DeltaLossV |         -0.0514 |\n",
      "|           Entropy |           0.762 |\n",
      "|                KL |         0.00827 |\n",
      "|          ClipFrac |           0.121 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            82.7 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              21 |\n",
      "|      AverageEpRet |           0.466 |\n",
      "|          StdEpRet |           0.811 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.28 |\n",
      "|      AverageVVals |            0.51 |\n",
      "|          StdVVals |           0.356 |\n",
      "|          MaxVVals |             1.3 |\n",
      "|          MinVVals |            -1.1 |\n",
      "| TotalEnvInteracts |         1.1e+05 |\n",
      "|            LossPi |        7.63e-09 |\n",
      "|             LossV |           0.438 |\n",
      "|       DeltaLossPi |         -0.0231 |\n",
      "|        DeltaLossV |         -0.0486 |\n",
      "|           Entropy |           0.753 |\n",
      "|                KL |          0.0104 |\n",
      "|          ClipFrac |           0.106 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            86.8 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              22 |\n",
      "|      AverageEpRet |           0.477 |\n",
      "|          StdEpRet |           0.807 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.25 |\n",
      "|      AverageVVals |           0.496 |\n",
      "|          StdVVals |           0.387 |\n",
      "|          MaxVVals |            1.43 |\n",
      "|          MinVVals |          -0.933 |\n",
      "| TotalEnvInteracts |        1.15e+05 |\n",
      "|            LossPi |        2.71e-08 |\n",
      "|             LossV |           0.439 |\n",
      "|       DeltaLossPi |         -0.0201 |\n",
      "|        DeltaLossV |         -0.0461 |\n",
      "|           Entropy |           0.699 |\n",
      "|                KL |         0.00713 |\n",
      "|          ClipFrac |          0.0912 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            91.2 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              23 |\n",
      "|      AverageEpRet |           0.495 |\n",
      "|          StdEpRet |           0.787 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.31 |\n",
      "|      AverageVVals |            0.52 |\n",
      "|          StdVVals |           0.391 |\n",
      "|          MaxVVals |            1.59 |\n",
      "|          MinVVals |          -0.928 |\n",
      "| TotalEnvInteracts |         1.2e+05 |\n",
      "|            LossPi |        3.43e-09 |\n",
      "|             LossV |           0.408 |\n",
      "|       DeltaLossPi |         -0.0206 |\n",
      "|        DeltaLossV |         -0.0386 |\n",
      "|           Entropy |           0.651 |\n",
      "|                KL |         0.00881 |\n",
      "|          ClipFrac |          0.0862 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            95.3 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              24 |\n",
      "|      AverageEpRet |           0.552 |\n",
      "|          StdEpRet |           0.761 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.27 |\n",
      "|      AverageVVals |           0.535 |\n",
      "|          StdVVals |           0.367 |\n",
      "|          MaxVVals |            1.39 |\n",
      "|          MinVVals |          -0.865 |\n",
      "| TotalEnvInteracts |        1.25e+05 |\n",
      "|            LossPi |        1.95e-08 |\n",
      "|             LossV |            0.38 |\n",
      "|       DeltaLossPi |         -0.0198 |\n",
      "|        DeltaLossV |         -0.0369 |\n",
      "|           Entropy |           0.616 |\n",
      "|                KL |         0.00636 |\n",
      "|          ClipFrac |           0.074 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            99.5 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              25 |\n",
      "|      AverageEpRet |           0.568 |\n",
      "|          StdEpRet |           0.753 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.28 |\n",
      "|      AverageVVals |           0.583 |\n",
      "|          StdVVals |           0.367 |\n",
      "|          MaxVVals |            1.41 |\n",
      "|          MinVVals |          -0.808 |\n",
      "| TotalEnvInteracts |         1.3e+05 |\n",
      "|            LossPi |       -1.13e-08 |\n",
      "|             LossV |           0.374 |\n",
      "|       DeltaLossPi |         -0.0198 |\n",
      "|        DeltaLossV |         -0.0372 |\n",
      "|           Entropy |           0.548 |\n",
      "|                KL |         0.00818 |\n",
      "|          ClipFrac |          0.0842 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             103 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              26 |\n",
      "|      AverageEpRet |           0.573 |\n",
      "|          StdEpRet |           0.761 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.21 |\n",
      "|      AverageVVals |           0.577 |\n",
      "|          StdVVals |           0.377 |\n",
      "|          MaxVVals |             1.4 |\n",
      "|          MinVVals |          -0.937 |\n",
      "| TotalEnvInteracts |        1.35e+05 |\n",
      "|            LossPi |        -2.1e-09 |\n",
      "|             LossV |           0.375 |\n",
      "|       DeltaLossPi |         -0.0189 |\n",
      "|        DeltaLossV |         -0.0421 |\n",
      "|           Entropy |           0.527 |\n",
      "|                KL |         0.00976 |\n",
      "|          ClipFrac |          0.0896 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             107 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 3 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              27 |\n",
      "|      AverageEpRet |             0.6 |\n",
      "|          StdEpRet |            0.73 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.25 |\n",
      "|      AverageVVals |           0.604 |\n",
      "|          StdVVals |           0.379 |\n",
      "|          MaxVVals |             1.9 |\n",
      "|          MinVVals |           -0.98 |\n",
      "| TotalEnvInteracts |         1.4e+05 |\n",
      "|            LossPi |       -5.72e-09 |\n",
      "|             LossV |           0.363 |\n",
      "|       DeltaLossPi |         -0.0208 |\n",
      "|        DeltaLossV |         -0.0417 |\n",
      "|           Entropy |           0.543 |\n",
      "|                KL |          0.0126 |\n",
      "|          ClipFrac |           0.081 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             111 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              28 |\n",
      "|      AverageEpRet |           0.589 |\n",
      "|          StdEpRet |           0.751 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.19 |\n",
      "|      AverageVVals |           0.611 |\n",
      "|          StdVVals |           0.351 |\n",
      "|          MaxVVals |            1.49 |\n",
      "|          MinVVals |           -0.98 |\n",
      "| TotalEnvInteracts |        1.45e+05 |\n",
      "|            LossPi |        -1.6e-08 |\n",
      "|             LossV |           0.368 |\n",
      "|       DeltaLossPi |         -0.0198 |\n",
      "|        DeltaLossV |         -0.0375 |\n",
      "|           Entropy |           0.503 |\n",
      "|                KL |          0.0111 |\n",
      "|          ClipFrac |          0.0922 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             115 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              29 |\n",
      "|      AverageEpRet |           0.579 |\n",
      "|          StdEpRet |           0.757 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |             3.2 |\n",
      "|      AverageVVals |           0.616 |\n",
      "|          StdVVals |           0.365 |\n",
      "|          MaxVVals |            1.24 |\n",
      "|          MinVVals |          -0.971 |\n",
      "| TotalEnvInteracts |         1.5e+05 |\n",
      "|            LossPi |        4.01e-09 |\n",
      "|             LossV |           0.372 |\n",
      "|       DeltaLossPi |         -0.0176 |\n",
      "|        DeltaLossV |         -0.0297 |\n",
      "|           Entropy |           0.469 |\n",
      "|                KL |         0.00787 |\n",
      "|          ClipFrac |          0.0866 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             119 |\n",
      "---------------------------------------\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: spinupPpo/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |              30 |\n",
      "|      AverageEpRet |           0.596 |\n",
      "|          StdEpRet |           0.745 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.18 |\n",
      "|      AverageVVals |           0.601 |\n",
      "|          StdVVals |            0.37 |\n",
      "|          MaxVVals |            1.22 |\n",
      "|          MinVVals |           -1.07 |\n",
      "| TotalEnvInteracts |        1.55e+05 |\n",
      "|            LossPi |        9.35e-09 |\n",
      "|             LossV |           0.357 |\n",
      "|       DeltaLossPi |         -0.0161 |\n",
      "|        DeltaLossV |          -0.033 |\n",
      "|           Entropy |           0.442 |\n",
      "|                KL |         0.00573 |\n",
      "|          ClipFrac |           0.061 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             123 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              31 |\n",
      "|      AverageEpRet |           0.622 |\n",
      "|          StdEpRet |           0.717 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.18 |\n",
      "|      AverageVVals |           0.611 |\n",
      "|          StdVVals |           0.382 |\n",
      "|          MaxVVals |            1.44 |\n",
      "|          MinVVals |           -1.16 |\n",
      "| TotalEnvInteracts |         1.6e+05 |\n",
      "|            LossPi |       -1.16e-08 |\n",
      "|             LossV |           0.329 |\n",
      "|       DeltaLossPi |         -0.0166 |\n",
      "|        DeltaLossV |         -0.0333 |\n",
      "|           Entropy |           0.392 |\n",
      "|                KL |           0.011 |\n",
      "|          ClipFrac |          0.0884 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             128 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "|             Epoch |              32 |\n",
      "|      AverageEpRet |           0.637 |\n",
      "|          StdEpRet |           0.707 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.17 |\n",
      "|      AverageVVals |           0.633 |\n",
      "|          StdVVals |           0.368 |\n",
      "|          MaxVVals |            1.32 |\n",
      "|          MinVVals |          -0.984 |\n",
      "| TotalEnvInteracts |        1.65e+05 |\n",
      "|            LossPi |       -6.96e-09 |\n",
      "|             LossV |           0.325 |\n",
      "|       DeltaLossPi |         -0.0152 |\n",
      "|        DeltaLossV |         -0.0363 |\n",
      "|           Entropy |           0.411 |\n",
      "|                KL |         0.00558 |\n",
      "|          ClipFrac |          0.0646 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             132 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              33 |\n",
      "|      AverageEpRet |           0.639 |\n",
      "|          StdEpRet |            0.72 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.14 |\n",
      "|      AverageVVals |           0.655 |\n",
      "|          StdVVals |           0.366 |\n",
      "|          MaxVVals |            1.33 |\n",
      "|          MinVVals |           -0.94 |\n",
      "| TotalEnvInteracts |         1.7e+05 |\n",
      "|            LossPi |        1.49e-08 |\n",
      "|             LossV |           0.332 |\n",
      "|       DeltaLossPi |         -0.0163 |\n",
      "|        DeltaLossV |         -0.0341 |\n",
      "|           Entropy |           0.386 |\n",
      "|                KL |         0.00965 |\n",
      "|          ClipFrac |          0.0716 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             136 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              34 |\n",
      "|      AverageEpRet |           0.615 |\n",
      "|          StdEpRet |            0.72 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.17 |\n",
      "|      AverageVVals |           0.631 |\n",
      "|          StdVVals |            0.39 |\n",
      "|          MaxVVals |            1.78 |\n",
      "|          MinVVals |          -0.845 |\n",
      "| TotalEnvInteracts |        1.75e+05 |\n",
      "|            LossPi |        3.43e-09 |\n",
      "|             LossV |           0.341 |\n",
      "|       DeltaLossPi |         -0.0167 |\n",
      "|        DeltaLossV |         -0.0393 |\n",
      "|           Entropy |           0.365 |\n",
      "|                KL |         0.00813 |\n",
      "|          ClipFrac |          0.0682 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             140 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              35 |\n",
      "|      AverageEpRet |           0.652 |\n",
      "|          StdEpRet |           0.703 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.16 |\n",
      "|      AverageVVals |           0.645 |\n",
      "|          StdVVals |            0.35 |\n",
      "|          MaxVVals |            1.43 |\n",
      "|          MinVVals |              -1 |\n",
      "| TotalEnvInteracts |         1.8e+05 |\n",
      "|            LossPi |        3.48e-09 |\n",
      "|             LossV |           0.325 |\n",
      "|       DeltaLossPi |         -0.0131 |\n",
      "|        DeltaLossV |         -0.0315 |\n",
      "|           Entropy |           0.342 |\n",
      "|                KL |         0.00501 |\n",
      "|          ClipFrac |          0.0716 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             144 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              36 |\n",
      "|      AverageEpRet |           0.665 |\n",
      "|          StdEpRet |           0.691 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.16 |\n",
      "|      AverageVVals |           0.662 |\n",
      "|          StdVVals |           0.352 |\n",
      "|          MaxVVals |            1.39 |\n",
      "|          MinVVals |           -1.07 |\n",
      "| TotalEnvInteracts |        1.85e+05 |\n",
      "|            LossPi |       -1.62e-08 |\n",
      "|             LossV |           0.308 |\n",
      "|       DeltaLossPi |          -0.014 |\n",
      "|        DeltaLossV |         -0.0259 |\n",
      "|           Entropy |            0.32 |\n",
      "|                KL |         0.00573 |\n",
      "|          ClipFrac |           0.062 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             148 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              37 |\n",
      "|      AverageEpRet |           0.679 |\n",
      "|          StdEpRet |           0.678 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.16 |\n",
      "|      AverageVVals |           0.684 |\n",
      "|          StdVVals |           0.335 |\n",
      "|          MaxVVals |            1.53 |\n",
      "|          MinVVals |          -0.885 |\n",
      "| TotalEnvInteracts |         1.9e+05 |\n",
      "|            LossPi |       -7.63e-10 |\n",
      "|             LossV |           0.301 |\n",
      "|       DeltaLossPi |         -0.0143 |\n",
      "|        DeltaLossV |         -0.0208 |\n",
      "|           Entropy |           0.326 |\n",
      "|                KL |         0.00796 |\n",
      "|          ClipFrac |          0.0546 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             152 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              38 |\n",
      "|      AverageEpRet |           0.657 |\n",
      "|          StdEpRet |           0.693 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.16 |\n",
      "|      AverageVVals |           0.669 |\n",
      "|          StdVVals |           0.342 |\n",
      "|          MaxVVals |            1.39 |\n",
      "|          MinVVals |           -1.05 |\n",
      "| TotalEnvInteracts |        1.95e+05 |\n",
      "|            LossPi |       -1.45e-08 |\n",
      "|             LossV |           0.304 |\n",
      "|       DeltaLossPi |         -0.0129 |\n",
      "|        DeltaLossV |         -0.0247 |\n",
      "|           Entropy |             0.3 |\n",
      "|                KL |         0.00729 |\n",
      "|          ClipFrac |          0.0658 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             156 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              39 |\n",
      "|      AverageEpRet |           0.694 |\n",
      "|          StdEpRet |           0.664 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.16 |\n",
      "|      AverageVVals |           0.685 |\n",
      "|          StdVVals |           0.339 |\n",
      "|          MaxVVals |            1.41 |\n",
      "|          MinVVals |           -0.81 |\n",
      "| TotalEnvInteracts |           2e+05 |\n",
      "|            LossPi |        1.53e-09 |\n",
      "|             LossV |             0.3 |\n",
      "|       DeltaLossPi |         -0.0122 |\n",
      "|        DeltaLossV |          -0.036 |\n",
      "|           Entropy |           0.291 |\n",
      "|                KL |         0.00989 |\n",
      "|          ClipFrac |          0.0664 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             161 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 3 steps.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: spinupPpo/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |              40 |\n",
      "|      AverageEpRet |           0.724 |\n",
      "|          StdEpRet |           0.641 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.17 |\n",
      "|      AverageVVals |           0.703 |\n",
      "|          StdVVals |           0.327 |\n",
      "|          MaxVVals |            1.64 |\n",
      "|          MinVVals |           -1.06 |\n",
      "| TotalEnvInteracts |        2.05e+05 |\n",
      "|            LossPi |        -1.6e-08 |\n",
      "|             LossV |           0.284 |\n",
      "|       DeltaLossPi |         -0.0126 |\n",
      "|        DeltaLossV |         -0.0291 |\n",
      "|           Entropy |           0.284 |\n",
      "|                KL |         0.00776 |\n",
      "|          ClipFrac |          0.0594 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             165 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              41 |\n",
      "|      AverageEpRet |           0.707 |\n",
      "|          StdEpRet |            0.65 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.16 |\n",
      "|      AverageVVals |           0.722 |\n",
      "|          StdVVals |            0.31 |\n",
      "|          MaxVVals |            1.61 |\n",
      "|          MinVVals |          -0.893 |\n",
      "| TotalEnvInteracts |         2.1e+05 |\n",
      "|            LossPi |       -6.87e-09 |\n",
      "|             LossV |           0.264 |\n",
      "|       DeltaLossPi |          -0.011 |\n",
      "|        DeltaLossV |         -0.0188 |\n",
      "|           Entropy |           0.291 |\n",
      "|                KL |         0.00718 |\n",
      "|          ClipFrac |          0.0628 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             170 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              42 |\n",
      "|      AverageEpRet |           0.667 |\n",
      "|          StdEpRet |            0.69 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.14 |\n",
      "|      AverageVVals |           0.706 |\n",
      "|          StdVVals |           0.327 |\n",
      "|          MaxVVals |            1.32 |\n",
      "|          MinVVals |          -0.718 |\n",
      "| TotalEnvInteracts |        2.15e+05 |\n",
      "|            LossPi |       -1.22e-08 |\n",
      "|             LossV |           0.319 |\n",
      "|       DeltaLossPi |         -0.0137 |\n",
      "|        DeltaLossV |         -0.0328 |\n",
      "|           Entropy |           0.287 |\n",
      "|                KL |         0.00586 |\n",
      "|          ClipFrac |          0.0598 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             173 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              43 |\n",
      "|      AverageEpRet |           0.695 |\n",
      "|          StdEpRet |           0.664 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.18 |\n",
      "|      AverageVVals |           0.678 |\n",
      "|          StdVVals |           0.334 |\n",
      "|          MaxVVals |             1.5 |\n",
      "|          MinVVals |          -0.893 |\n",
      "| TotalEnvInteracts |         2.2e+05 |\n",
      "|            LossPi |        -2.4e-08 |\n",
      "|             LossV |             0.3 |\n",
      "|       DeltaLossPi |         -0.0124 |\n",
      "|        DeltaLossV |         -0.0339 |\n",
      "|           Entropy |           0.301 |\n",
      "|                KL |          0.0107 |\n",
      "|          ClipFrac |           0.083 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             177 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              44 |\n",
      "|      AverageEpRet |           0.687 |\n",
      "|          StdEpRet |           0.672 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.15 |\n",
      "|      AverageVVals |           0.716 |\n",
      "|          StdVVals |           0.303 |\n",
      "|          MaxVVals |             1.6 |\n",
      "|          MinVVals |          -0.749 |\n",
      "| TotalEnvInteracts |        2.25e+05 |\n",
      "|            LossPi |        4.58e-09 |\n",
      "|             LossV |           0.309 |\n",
      "|       DeltaLossPi |         -0.0114 |\n",
      "|        DeltaLossV |         -0.0293 |\n",
      "|           Entropy |           0.287 |\n",
      "|                KL |         0.00637 |\n",
      "|          ClipFrac |          0.0546 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             181 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 3 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              45 |\n",
      "|      AverageEpRet |           0.663 |\n",
      "|          StdEpRet |           0.692 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.16 |\n",
      "|      AverageVVals |           0.684 |\n",
      "|          StdVVals |           0.323 |\n",
      "|          MaxVVals |            1.44 |\n",
      "|          MinVVals |          -0.922 |\n",
      "| TotalEnvInteracts |         2.3e+05 |\n",
      "|            LossPi |        7.53e-09 |\n",
      "|             LossV |            0.32 |\n",
      "|       DeltaLossPi |         -0.0123 |\n",
      "|        DeltaLossV |         -0.0213 |\n",
      "|           Entropy |           0.292 |\n",
      "|                KL |         0.00852 |\n",
      "|          ClipFrac |          0.0714 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             185 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              46 |\n",
      "|      AverageEpRet |           0.689 |\n",
      "|          StdEpRet |           0.672 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.15 |\n",
      "|      AverageVVals |           0.666 |\n",
      "|          StdVVals |           0.333 |\n",
      "|          MaxVVals |            1.45 |\n",
      "|          MinVVals |           -1.01 |\n",
      "| TotalEnvInteracts |        2.35e+05 |\n",
      "|            LossPi |       -5.72e-10 |\n",
      "|             LossV |           0.303 |\n",
      "|       DeltaLossPi |          -0.013 |\n",
      "|        DeltaLossV |         -0.0249 |\n",
      "|           Entropy |           0.278 |\n",
      "|                KL |           0.013 |\n",
      "|          ClipFrac |          0.0686 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             189 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              47 |\n",
      "|      AverageEpRet |           0.699 |\n",
      "|          StdEpRet |            0.66 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.15 |\n",
      "|      AverageVVals |           0.701 |\n",
      "|          StdVVals |           0.321 |\n",
      "|          MaxVVals |             1.3 |\n",
      "|          MinVVals |          -0.828 |\n",
      "| TotalEnvInteracts |         2.4e+05 |\n",
      "|            LossPi |       -1.18e-08 |\n",
      "|             LossV |           0.285 |\n",
      "|       DeltaLossPi |         -0.0119 |\n",
      "|        DeltaLossV |         -0.0267 |\n",
      "|           Entropy |           0.264 |\n",
      "|                KL |          0.0117 |\n",
      "|          ClipFrac |          0.0716 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             193 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              48 |\n",
      "|      AverageEpRet |           0.703 |\n",
      "|          StdEpRet |           0.649 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.18 |\n",
      "|      AverageVVals |           0.688 |\n",
      "|          StdVVals |           0.351 |\n",
      "|          MaxVVals |            1.38 |\n",
      "|          MinVVals |          -0.914 |\n",
      "| TotalEnvInteracts |        2.45e+05 |\n",
      "|            LossPi |       -1.11e-08 |\n",
      "|             LossV |           0.264 |\n",
      "|       DeltaLossPi |        -0.00984 |\n",
      "|        DeltaLossV |         -0.0183 |\n",
      "|           Entropy |           0.275 |\n",
      "|                KL |         0.00786 |\n",
      "|          ClipFrac |          0.0678 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             198 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: spinupPpo/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |              49 |\n",
      "|      AverageEpRet |           0.714 |\n",
      "|          StdEpRet |           0.645 |\n",
      "|          MaxEpRet |               1 |\n",
      "|          MinEpRet |              -1 |\n",
      "|             EpLen |            3.18 |\n",
      "|      AverageVVals |           0.717 |\n",
      "|          StdVVals |           0.318 |\n",
      "|          MaxVVals |            1.38 |\n",
      "|          MinVVals |           -1.24 |\n",
      "| TotalEnvInteracts |         2.5e+05 |\n",
      "|            LossPi |        6.87e-09 |\n",
      "|             LossV |           0.278 |\n",
      "|       DeltaLossPi |         -0.0138 |\n",
      "|        DeltaLossV |         -0.0227 |\n",
      "|           Entropy |           0.281 |\n",
      "|                KL |          0.0128 |\n",
      "|          ClipFrac |             0.1 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |             202 |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ac_kwargs = dict(hidden_sizes=[64,64], activation=tf.nn.relu)\n",
    "\n",
    "logger_kwargs = dict(output_dir='spinupPpo', exp_name='experiment')\n",
    "\n",
    "ppo(env_fn=env_fn, ac_kwargs=ac_kwargs, steps_per_epoch=5000, epochs=50, logger_kwargs=logger_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pontos a se observar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "No logger do treinamento feito acima, é importante se atentar a alguns valores de retorno. Para uma Epóca N, podemos analisar o <code>AverageEpRet</code>, que indica o valor médio de retorno dos episódios. Para a época 9, por exemplo, temos uma média de -0.152, sendo que o máximo de retorno é 1 (quando o bot ganha) e o mínimo é -1, que é quando o bot perde o jogo. Uma média de -0.152 indica que o bot ainda está perdendo mais vezes do que ganhando, o que nos leva a conclusão de que é necessário mais épocas de treinamento, já que, idealmente queremos, no máximo, empatar e, se possível, nunca perder.\n",
    "\n",
    "Ainda, podemos analisar o EpLen, que indica uma média da duração do episódio, ou seja, quantas vezes o nosso bot jogou, em média, cada jogo. Considerando que ele joga em segundo, o bot pode jogar no máximo 4 vezes, preenchendo os 9 campos, resultando em um empate, e, no mínimo, 2 vezes, considerando que ele não evitou a primeira chance do outro player ganhar. Interessante analisar que o EpLen, no ínicio, vai crescendo junto com o AverageEpRet, o que indica que o bot aprende primeiro a evitar as derrotas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao final das 50 épocas de treinamento, o AverageEpRet encontra-se em 0.714, o que corresponde a uma média boa e próxima de 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando nosso bot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar nosso bot treinado com o PPO, utilizamos a função <code>load_policy</code>, que carrega os pesos e váriaveis salvos no treinamento nos arquivos locais. Vamos carregar nosso modelo para obter a função <code>get_action</code>, que é a função que, ao passarmos um tabuleiro, retorna outro tabuleiro com a jogada que o bot tomou a partir daquele estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/raphacosta/Desktop/Insper7/machineLearning/projeto/spinningup/spinup/utils/logx.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./spinupPpo/simple_save/variables/variables\n",
      "Using default action op.\n"
     ]
    }
   ],
   "source": [
    "from spinup.utils.test_policy import load_policy, run_policy\n",
    "_, get_action = load_policy('./spinupPpo')\n",
    "env = env_fn()\n",
    "#run_policy(env, get_action, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de podermos testar nosso bot com a função <code>run_policy</code> da biblioteca do OpenAI, iremos fazer nossa própria função de teste para fins didáticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na função play, passaremos o argumento n, que representa apenas quantas jogos serão feitos. Para cada jogo, iniciamos chamando a função <code>reset</code> do ambiente, para iniciarmos a matriz e fazer a primeira jogada do jogador 1. \n",
    "\n",
    "Assim, iniciamos a váriavel d, que indica se o jogo terminou ou não. Enquanto o jogo não terminar, efetuamos jogadas com a função <code>get_action</code>. Após a <code>get_action</code>, indicamos ao ambiente a jogada feita pelo bot com a função <code>step</code>, que atualiza o estado do tabuleiro e verifica se há um ganhador, e, se não houver, realiza a jogada do jogador 1. Esta função retorna 3 principais valores: \n",
    "- <code>o</code>, que representa o estado do tabuleiro;\n",
    "- <code>r</code>, que representa a recompensa do Agente para aquela ação tomada;\n",
    "- <code>d</code>, que inidica se o jogo terminou;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(n=1):\n",
    "    for i in range(n):\n",
    "        o = env.reset()\n",
    "        env.render()\n",
    "        d = False\n",
    "        while not d:\n",
    "            a = get_action(o)\n",
    "            o, r, d, _ = env.step(a)\n",
    "            env.render()\n",
    "        if(r == 0):\n",
    "            print(\"Deu velha\")\n",
    "        elif(r == 1):\n",
    "            print(\"Bot ganhou\")\n",
    "        elif(r == -1):\n",
    "            print(\"Bot Perdeu\")\n",
    "        print(\"-------------------\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TicTacToe com Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a o Deep Q-Learning utilizaremos a biblioteca do keras-rl. Como dito anteriormente, nosso ambiente foi feito com base na biblioteca do keras, portanto, não devemos ter muito problema para utilizar sua implementação. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe <code>ModelIntervalCheckpoint</code>, como o próprio nome sugere, é responsável por salvar certos pontos de checkpoint durante o treinamento e, também é ela quem salva localmente os arquivos com os pesos encontrados para a rede neural em cada episódio, o que também tinhamos na execução do PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelIntervalCheckpoint(Callback):\n",
    "    def __init__(self, interval, verbose=0):\n",
    "        super(ModelIntervalCheckpoint, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.step = 0\n",
    "\n",
    "        self.rewards = []\n",
    "        self.last_max = -1\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "\n",
    "    def on_step_begin(self, step, logs):\n",
    "        if self.step % self.interval == 0:\n",
    "            if len(self.rewards) > 0:\n",
    "                mean_reward = np.nanmean(self.rewards, axis=0)\n",
    "                if mean_reward > self.last_max:\n",
    "                    filename = 'saved-weights/%s.h5f' % mean_reward\n",
    "                    print(\"\\nSaving model checkpoint with mean reward %s to %s\" % (mean_reward, filename))\n",
    "\n",
    "                    self.model.save_weights(filename, overwrite=True)\n",
    "\n",
    "                    self.last_max = mean_reward\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "\n",
    "        self.rewards.append(logs['reward'])\n",
    "        self.step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente criamos uma váriavel <code>env</code> em referência ao nosso ambiente, o <code>TicTacToeEnv</code>. A váriavel <code>model</code> será responsável por indicar o formato da Rede Neural que utilizaremos durante o experimento. \n",
    "\n",
    "Um conceito, que não há no PPO, é o de guardar os estados que o Agente ja passou, evitando que ele esqueça totalmente onde ja passou em passos anteriores. Para isso, criamos a váriavel <code>memory</code>. Ainda, criamos a <code>policy</code>, que diz respeito a policy utilizada pelo Agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, criamos um objeto do da classe <code>DQNAgent</code>, que recebe 7 argumentos obrigatórios:\n",
    "- <code>model</code>: modelo de rede neural que utilizaremos no experimento.\n",
    "- <code>nb_actions</code>: número de ações possíveis para o Agente (no nosso caso, 9, pois existem 9 posições possíveis para ele jogar)\n",
    "- <code>memory</code>: memória das jogadas anteriores\n",
    "- <code>target_model_update</code>: hiperparâmetro do modelo, que inidica a frequência com que atualizaremos os valores na rede neural. Para casos em que a váriavel está entre 0 e 1, estamos em Soft Update, e para casos em que a váriavel é maior que 1, estamos em Hard Update. No soft update, atualizamos os valores da rede neural por partes, enquanto que no hard update atualizamos a rede neural como um todo a cada <code>target_model_update</code> passos.\n",
    "- <code>policy</code>: policy utilizada pelo Agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para efetivamente treinarmos o modelo, precisamos chamar 2 funções da biblioteca:\n",
    "\n",
    "1) <code>Compile</code>: compila o Agente que será utilizado, determina um Optmizer para ser utilizado no treinamento e recebe a métrica sobre o qual queremos analisar o desempenho. No nosso caso, utilizaremos o mae, mean absolut error.\n",
    "\n",
    "2) <code>fit</code>: Treina o Agente sobre o ambiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dqn(env):\n",
    "    nb_actions = env.action_space.n\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "    memory = SequentialMemory(limit=5000000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    log_interval = 10000\n",
    "\n",
    "    dqn = DQNAgent(\n",
    "        model=model,\n",
    "        nb_actions=nb_actions,\n",
    "        memory=memory,\n",
    "        target_model_update=1e-2,\n",
    "        policy=policy\n",
    "    )\n",
    "\n",
    "    dqn.compile(Adam(lr=1e-5), metrics=['accuracy', 'mae'])\n",
    "\n",
    "    return dqn\n",
    "\n",
    "def predict(dqn, board_state, model_path):\n",
    "    env = TicTacToeEnv(predict_for=board_state)\n",
    "\n",
    "    dqn.load_weights(model_path)\n",
    "\n",
    "    dqn.test(env, nb_episodes=1, visualize=False, verbose=0)\n",
    "\n",
    "    return dqn.recent_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeEnv()\n",
    "\n",
    "dqn = build_dqn(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atenção!!! O treinamento demora, não execute-a se você não deseja sobrepor o treinamento que ja fizemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1,\n",
    "    callbacks=[ModelIntervalCheckpoint(interval=log_interval)],\n",
    "    log_interval=log_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar, basta escolher o arquivo de pesos gerado com a maior média de retorno, que corresponde ao <code>AverageEpRet</code> no ambito do PPO. Portanto, a principio queremos um valor maior que 0, que inidicaria, novamente, que o Agente ganhou mais partidas do que perdeu. Porém, quando colocamos este Agente para treinar com um nb_steps=5000000, conseguimos com que esta média fosse de -0.141, o que pode indicar que, utilizando PPO, conseguimos um melhor resultado mais rápido, ja que, para treinar o PPO, precisamos de apenas 50 épocas para chegar em uma média de 0.751."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playDql(n=1):\n",
    "    for i in range(n):\n",
    "        o = env.reset()\n",
    "        env.render()\n",
    "        d = False\n",
    "        while not d:\n",
    "            board_state = np.array([\n",
    "                o[0:3],\n",
    "                o[3:6],\n",
    "                o[6:]\n",
    "            ])\n",
    "            a = predict(dqn, board_state, 'saved-weights/-0.141.h5f')\n",
    "            o, r, d, _ = env.step(a)\n",
    "            env.render()\n",
    "        if(r == 0):\n",
    "            print(\"Deu velha\")\n",
    "        elif(r == 1):\n",
    "            print(\"Bot ganhou\")\n",
    "        elif(r == -1):\n",
    "            print(\"Bot Perdeu\")\n",
    "        print(\"-------------------\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[[1 1 0]\n",
      " [0 2 0]\n",
      " [0 0 0]]\n",
      "[[1 1 2]\n",
      " [1 2 0]\n",
      " [0 0 0]]\n",
      "[[1 1 2]\n",
      " [1 2 0]\n",
      " [2 0 0]]\n",
      "Bot ganhou\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "playDql(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO x DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O nosso teste final, é claro, colocar o nosso Agente PPO treinado para jogar contra o Agente do DQN. Façam suas apostas.\n",
    "\n",
    "Para jogar, utilizaremos a função <code>stepX1</code>, feita no <code>TicTacToeEnv</code>. Esta função difere da <code>step</code> utilizada anteriormente, pois não faremos um movimento aleatório para o jogador 1, apenas colocará 1 ou 2, dependendo da vez de quem for, em uma posição do tabuleiro. \n",
    "\n",
    "Portanto, nossa função <code>stepX1</code> funciona da seguinte forma:\n",
    "\n",
    "- Recebe uma posição da matriz que deve ser jogada;\n",
    "\n",
    "- Transforma essa posição em uma coordenada relativa x,y;\n",
    "\n",
    "- Escreve na matriz 1 ou 2 na posição recebida;\n",
    "\n",
    "- Verifica se há um ganhador. A função de verificar um ganhador pode retornar 4 valores diferentes:\n",
    "    - 0, se a partida ainda não acabou;\n",
    "    - 1, se o jogador 1 ganhou;\n",
    "    - 2, se o jogador 2 ganhou;\n",
    "    - 3, se a partida empatou\n",
    "    \n",
    "- Por fim, a função retorna o tabuleiro atual após a jogada feita e o resultado da chamada da função winner.\n",
    "\n",
    "Desta forma, faremos 2 testes diferentes: \n",
    "\n",
    "- Colocaremos, primeiro, o PPO como jogador número 1 e o DQN como jogador número 2.\n",
    "- Depois, faremos o inverso, colocaremos o DQN como jogador número 1 e o PPO como jogador número 2.\n",
    "\n",
    "Desta forma, veremos se, em algum dos agentes treinados, faz diferença ele começar jogando ou não. Esperamos que não ja que, no treinamento de ambos, os bots sempre jogavam em segundo.\n",
    "\n",
    "Ainda, para jogar com o PPO, utilizamos a função <code>get_action</code> e, para jogar com o DQN utilizamos a função <code>predict</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppoxdqn(train=False, visualize=False):\n",
    "    turn = 1\n",
    "    \n",
    "    o = env.resetX1()\n",
    "    if(visualize):\n",
    "        env.render()\n",
    "        print(\"PPO: 1\")\n",
    "        print(\"DQN: 2\")\n",
    "        print(\"-------------------\")\n",
    "        \n",
    "    w = 0\n",
    "    while w == 0:\n",
    "        if(turn == 1):\n",
    "            if(visualize):\n",
    "                print(\"PPO turn\")\n",
    "            a = get_action(o)\n",
    "            try:\n",
    "                o, w = env.stepX1(a)\n",
    "            except:\n",
    "                print(a)\n",
    "                print(o)\n",
    "                raise ValueError(f\"Deu erro: {a}, {o}\")\n",
    "                \n",
    "            turn = 2\n",
    "\n",
    "        elif(turn == 2):\n",
    "            if(visualize):\n",
    "                print(\"DQN turn\")\n",
    "            board_state = np.array([\n",
    "                o[0:3],\n",
    "                o[3:6],\n",
    "                o[6:]\n",
    "            ])\n",
    "\n",
    "            a = predict(dqn, board_state, 'saved-weights/-0.141.h5f')\n",
    "            try:\n",
    "                o, w = env.stepX1(a)\n",
    "            except:\n",
    "                print(a)\n",
    "                print(o)\n",
    "                raise ValueError(f\"Deu erro: {a}, {o}\")\n",
    "#                 return -1\n",
    "            turn = 1\n",
    "\n",
    "        if(visualize):\n",
    "            env.render()\n",
    "            print(\"-------------------\")\n",
    "            \n",
    "    if(visualize):\n",
    "        if(w == 1):\n",
    "            print(\"PPO ganhou\")\n",
    "        elif(w == 2):\n",
    "            print(\"DQN ganhou\")\n",
    "        elif(w == 3):\n",
    "            print(\"Deu velha\")\n",
    "        \n",
    "        print(\"-------------------\")\n",
    "        \n",
    "    return w\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "PPO: 1\n",
      "DQN: 2\n",
      "-------------------\n",
      "PPO turn\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]]\n",
      "-------------------\n",
      "DQN turn\n",
      "[[0 0 0]\n",
      " [0 2 0]\n",
      " [0 0 1]]\n",
      "-------------------\n",
      "PPO turn\n",
      "[[0 0 0]\n",
      " [0 2 0]\n",
      " [1 0 1]]\n",
      "-------------------\n",
      "DQN turn\n",
      "[[0 0 0]\n",
      " [0 2 0]\n",
      " [1 2 1]]\n",
      "-------------------\n",
      "PPO turn\n",
      "[[1 0 0]\n",
      " [0 2 0]\n",
      " [1 2 1]]\n",
      "-------------------\n",
      "DQN turn\n",
      "[[1 2 0]\n",
      " [0 2 0]\n",
      " [1 2 1]]\n",
      "-------------------\n",
      "DQN ganhou\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppoxdqn(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      "[2 2 1 1 1 0 2 2 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Deu erro: 2, [2 2 1 1 1 0 2 2 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c60b3e693f9f>\u001b[0m in \u001b[0;36mppoxdqn\u001b[0;34m(train, visualize)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepX1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1e6df1e89a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwinners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppoxdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwinners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-c60b3e693f9f>\u001b[0m in \u001b[0;36mppoxdqn\u001b[0;34m(train, visualize)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Deu erro: {a}, {o}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mturn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Deu erro: 2, [2 2 1 1 1 0 2 2 1]"
     ]
    }
   ],
   "source": [
    "winners = []\n",
    "for i in range(300):\n",
    "    w = ppoxdqn(train=True, visualize=False)\n",
    "    winners.append(w)\n",
    "    print(w, end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Empate: \", winners.count(3)/len(winners))\n",
    "print(\"DQN winrate: \", winners.count(2)/len(winners))\n",
    "print(\"PPO winrate: \", winners.count(1)/len(winners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqnxppo(train=False, visualize=False):\n",
    "    turn = 1\n",
    "    \n",
    "    o = env.resetX1()\n",
    "    if(visualize):\n",
    "        env.render()\n",
    "        print(\"PPO: 1\")\n",
    "        print(\"DQN: 2\")\n",
    "        print(\"-------------------\")\n",
    "        \n",
    "    w = 0\n",
    "    while w == 0:\n",
    "        if(turn == 2):\n",
    "            if(visualize):\n",
    "                print(\"PPO turn\")\n",
    "            a = get_action(o)\n",
    "            try:\n",
    "                o, w = env.stepX1(a)\n",
    "            except:\n",
    "                return -1\n",
    "            turn = 2\n",
    "\n",
    "        elif(turn == 1):\n",
    "            if(visualize):\n",
    "                print(\"DQN turn\")\n",
    "            board_state = np.array([\n",
    "                o[0:3],\n",
    "                o[3:6],\n",
    "                o[6:]\n",
    "            ])\n",
    "\n",
    "            a = predict(dqn, board_state, 'saved-weights/-0.141.h5f')\n",
    "            try:\n",
    "                o, w = env.stepX1(a)\n",
    "            except:\n",
    "                return -1\n",
    "            turn = 1\n",
    "\n",
    "        if(visualize):\n",
    "            env.render()\n",
    "            print(\"-------------------\")\n",
    "            \n",
    "    if(visualize):\n",
    "        if(w == 1):\n",
    "            print(\"DQN ganhou\")\n",
    "        elif(w == 2):\n",
    "            print(\"PPO ganhou\")\n",
    "        elif(w == 3):\n",
    "            print(\"Deu velha\")\n",
    "        \n",
    "        print(\"-------------------\")\n",
    "        \n",
    "    return w\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqnxppo(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = []\n",
    "for i in range(50):\n",
    "    w = dqnxppo(train=True)\n",
    "    winners.append(w)\n",
    "    print(w, end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "print(not a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jogue contra os bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanxbot():\n",
    "    print(\"Escolha seu adversario: \")\n",
    "    print(\"1) PPO\")\n",
    "    print(\"2) DQN\")\n",
    "    \n",
    "    choice = int(input(\"Digite a opção desejada: \"))\n",
    "    \n",
    "    bot = \"PPO\"\n",
    "    \n",
    "    if(choice == 2):\n",
    "        bot = \"DQN\" \n",
    "        \n",
    "    print(f\"Bem vindo ao duelo, você duelará com {bot}\")\n",
    "    print(\"-------------------\")\n",
    "    \n",
    "    human_turn = 0\n",
    "    \n",
    "    print(\"Escolha:\")\n",
    "    print(\"0) Se deseja jogar primeiro\")\n",
    "    print(\"1) Se deseja que o bot comece jogando\")\n",
    "    human_turn = int(input(\"Digite a opção desejada: \"))\n",
    "    \n",
    "    if(human_turn == 0):\n",
    "        print(\"Voce escolheu jogar primeiro\")\n",
    "        print(\"Voce é o 1\")\n",
    "    else:\n",
    "        print(\"Voce escolheu jogar em segundo\") \n",
    "        print(\"Voce é o 2\")\n",
    "        \n",
    "    print(\"-------------------\")\n",
    "    \n",
    "    o = env.resetX1()\n",
    "    \n",
    "    env.render()\n",
    "\n",
    "    print(\"-------------------\")\n",
    "    \n",
    "    turn=0    \n",
    "    w = 0\n",
    "    \n",
    "    while w == 0:\n",
    "        if(turn == human_turn):\n",
    "            a = int(input(\"Em qual posição da matriz voce deseja jogar?[0-8] \"))\n",
    "            o, w = env.stepX1(a)\n",
    "        \n",
    "        else:\n",
    "            print(f\"{bot} turn\")\n",
    "            \n",
    "            if(bot == \"PPO\"):\n",
    "                a = get_action(o)\n",
    "                o, w = env.stepX1(a)\n",
    "            \n",
    "            elif(bot == \"DQN\"):\n",
    "                board_state = np.array([\n",
    "                    o[0:3],\n",
    "                    o[3:6],\n",
    "                    o[6:]\n",
    "                ])\n",
    "                a = predict(board_state, 'saved-weights/-0.141.h5f')\n",
    "                o, w = env.stepX1(a)\n",
    "                \n",
    "        turn = not turn\n",
    "                \n",
    "        env.render()\n",
    "        print(\"-------------------\")\n",
    "    if((w-1) == human_turn):\n",
    "        print(\"Voce ganhou!\")\n",
    "    elif(w == 3):\n",
    "        print(\"Deu velha\")\n",
    "    else:\n",
    "        print(\"Bot ganhou\")\n",
    "\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "humanxbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
