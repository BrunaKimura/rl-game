{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import Callback\n",
    "from spinup import ppo\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board_state = None\n",
    "    \n",
    "    def set_state(self, new_state):\n",
    "        \"\"\" 2d array of cell positions of the board. 0 = cell not occupied,\n",
    "            1 = cross occupies cell, 2 = nought occupies cell.\n",
    "            Example: [\n",
    "                [0, 0, 1],\n",
    "                [0, 0, 2],\n",
    "                [0, 0, 0]\n",
    "            ] \"\"\"\n",
    "\n",
    "        new_state = np.array(new_state)\n",
    "\n",
    "        assert new_state.shape == (len(new_state), len(new_state))\n",
    "\n",
    "        self.board_state = new_state\n",
    "\n",
    "        return self.board_state\n",
    "\n",
    "    def is_finished(self):\n",
    "        \"\"\" 0 = not finished, 1 = cross win, 2 = nought win, 3 = tie \"\"\"\n",
    "\n",
    "        # Are we tied?\n",
    "        if self.board_state.flatten().tolist().count(0) == 0:\n",
    "            return 3\n",
    "\n",
    "        # Stolen: https://codereview.stackexchange.com/a/24775\n",
    "        positions_groups = (\n",
    "            [[(x, y) for y in range(self.get_board_size())] for x in range(self.get_board_size())] +  # horizontals\n",
    "            [[(x, y) for x in range(self.get_board_size())] for y in range(self.get_board_size())] +  # verticals\n",
    "            [[(d, d) for d in range(self.get_board_size())]] +  # diagonal from top-left to bottom-right\n",
    "            [[(2-d, d) for d in range(self.get_board_size())]]  # diagonal from top-right to bottom-left\n",
    "        )\n",
    "        for positions in positions_groups:\n",
    "            values = [self.board_state[x][y] for (x, y) in positions]\n",
    "            if len(set(values)) == 1 and values[0]:\n",
    "                return values[0]\n",
    "\n",
    "        # Game isn't finished\n",
    "        return 0\n",
    "\n",
    "    def get_board_size(self):\n",
    "        return len(self.board_state)\n",
    "\n",
    "    def get_turn(self):\n",
    "        \"\"\" Returns 1 for crosses turn, 2 for noughts turn \"\"\"\n",
    "\n",
    "        flattened_list = self.board_state.flatten().tolist()\n",
    "\n",
    "        if flattened_list.count(1) > flattened_list.count(2):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def make_move(self, x, y):\n",
    "        \"\"\" Updates the state with the requested new occupied cell \"\"\"\n",
    "        x = int(x)\n",
    "        y = int(y)\n",
    "        \n",
    "        # Sanity check\n",
    "        assert x < self.get_board_size() and y < self.get_board_size()\n",
    "        assert self.board_state[y][x] == 0\n",
    "        assert self.is_finished() == 0\n",
    "\n",
    "        new_state = self.board_state.copy()\n",
    "        new_state[y][x] = self.get_turn()\n",
    "\n",
    "        return self.set_state(new_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def translate_position_to_xy(position, board_size=3):\n",
    "        \"\"\" Takes a single number and maps it to x, y coordinates.\n",
    "            Example: 8 = 2, 2 for a board_size of 3 \"\"\"\n",
    "\n",
    "        x = position % board_size\n",
    "        y = position / board_size\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    action_space = Discrete(3**2)\n",
    "\n",
    "    def __init__(self, board_size=3, predict_for=None):\n",
    "        self.board_size = board_size\n",
    "        self.predict_for = predict_for\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=np.array([0 for cell in range(self.board_size ** 2)]),\n",
    "            high=np.array([2 for cell in range(self.board_size ** 2)])\n",
    "        )\n",
    "        self.tictactoe = None\n",
    "        \n",
    "    def render(self):\n",
    "        if self.tictactoe is not None and self.tictactoe.board_state is not None:\n",
    "            print(self.tictactoe.board_state)\n",
    "                    \n",
    "    def reset(self):\n",
    "        if self.predict_for is not None:\n",
    "            self.tictactoe = TicTacToe()\n",
    "            self.tictactoe.set_state(self.predict_for)\n",
    "            return self.tictactoe.board_state.flatten()\n",
    "\n",
    "        self.tictactoe = TicTacToe()\n",
    "        self.tictactoe.set_state([\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "        move = self._get_random_move()\n",
    "\n",
    "        self.tictactoe.make_move(move[0], move[1])\n",
    "\n",
    "        return self.tictactoe.board_state.flatten()\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.predict_for is not None:\n",
    "            return self.tictactoe.board_state.flatten(), 0, True, {}\n",
    "\n",
    "        translated_action = TicTacToe.translate_position_to_xy(action)\n",
    "\n",
    "        try:\n",
    "            self.tictactoe.make_move(translated_action[0], translated_action[1])\n",
    "\n",
    "        except AssertionError:\n",
    "            return self.tictactoe.board_state.flatten(), -1, True, {}\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        winner = self.tictactoe.is_finished()\n",
    "        if winner == 0:\n",
    "            move = self._get_random_move()\n",
    "            self.tictactoe.make_move(move[0], move[1])\n",
    "\n",
    "            next_winner = self.tictactoe.is_finished()\n",
    "            if next_winner == 1:\n",
    "                reward = -1\n",
    "                done = True\n",
    "            elif next_winner == 3:\n",
    "                reward = 0\n",
    "                done = True\n",
    "\n",
    "        elif winner == 2:\n",
    "            reward = 1\n",
    "            done = True\n",
    "\n",
    "        elif winner == 3:\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        return self.tictactoe.board_state.flatten(), reward, done, {}\n",
    "\n",
    "    def _get_random_move(self):\n",
    "        assert self.tictactoe.is_finished() == 0\n",
    "\n",
    "        positions = []\n",
    "        for x in range(self.board_size):\n",
    "            for y in range(self.board_size):\n",
    "                if self.tictactoe.board_state[y][x] == 0:\n",
    "                    positions.append((x, y))\n",
    "\n",
    "        return positions[np.random.choice(len(positions), 1)[0]]\n",
    "\n",
    "    \n",
    "class ModelIntervalCheckpoint(Callback):\n",
    "    def __init__(self, interval, verbose=0):\n",
    "        super(ModelIntervalCheckpoint, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.step = 0\n",
    "\n",
    "        self.rewards = []\n",
    "        self.last_max = -1\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "\n",
    "    def on_step_begin(self, step, logs):\n",
    "        if self.step % self.interval == 0:\n",
    "            if len(self.rewards) > 0:\n",
    "                mean_reward = np.nanmean(self.rewards, axis=0)\n",
    "                if mean_reward > self.last_max:\n",
    "                    filename = 'saved-weights/%s.h5f' % mean_reward\n",
    "                    print(\"\\nSaving model checkpoint with mean reward %s to %s\" % (mean_reward, filename))\n",
    "\n",
    "                    self.model.save_weights(filename, overwrite=True)\n",
    "\n",
    "                    self.last_max = mean_reward\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "\n",
    "        self.rewards.append(logs['reward'])\n",
    "        self.step += 1\n",
    "        \n",
    "def predict(board_state, model_path):\n",
    "    env = TicTacToeEnv(predict_for=board_state)\n",
    "\n",
    "    dqn = build_dqn(env)\n",
    "\n",
    "    dqn.load_weights(model_path)\n",
    "\n",
    "    dqn.test(env, nb_episodes=1, visualize=False, verbose=0)\n",
    "\n",
    "    return dqn.recent_action\n",
    "\n",
    "\n",
    "def build_dqn(env):\n",
    "    nb_actions = env.action_space.n\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "    memory = SequentialMemory(limit=5000000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    log_interval = 10000\n",
    "\n",
    "    dqn = DQNAgent(\n",
    "        model=model,\n",
    "        nb_actions=nb_actions,\n",
    "        memory=memory,\n",
    "        nb_steps_warmup=1000,\n",
    "        enable_dueling_network=False,\n",
    "        target_model_update=1e-2,\n",
    "        policy=policy\n",
    "    )\n",
    "\n",
    "    dqn.compile(Adam(lr=1e-5), metrics=['accuracy', 'mae'])\n",
    "\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fn = lambda : gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Log dir spinupPpo already exists! Storing info there anyway.\n",
      "\u001b[32;1mLogging data to spinupPpo/progress.txt\u001b[0m\n",
      "\u001b[36;1mSaving config:\n",
      "\u001b[0m\n",
      "{\n",
      "    \"ac_kwargs\":\t{\n",
      "        \"activation\":\t\"relu\",\n",
      "        \"hidden_sizes\":\t[\n",
      "            64,\n",
      "            64\n",
      "        ]\n",
      "    },\n",
      "    \"actor_critic\":\t\"mlp_actor_critic\",\n",
      "    \"clip_ratio\":\t0.2,\n",
      "    \"env_fn\":\t\"<function <lambda> at 0x1081ef7b8>\",\n",
      "    \"epochs\":\t10,\n",
      "    \"exp_name\":\t\"experiment\",\n",
      "    \"gamma\":\t0.99,\n",
      "    \"lam\":\t0.97,\n",
      "    \"logger\":\t{\n",
      "        \"<spinup.utils.logx.EpochLogger object at 0x1278a4c18>\":\t{\n",
      "            \"epoch_dict\":\t{},\n",
      "            \"exp_name\":\t\"experiment\",\n",
      "            \"first_row\":\ttrue,\n",
      "            \"log_current_row\":\t{},\n",
      "            \"log_headers\":\t[],\n",
      "            \"output_dir\":\t\"spinupPpo\",\n",
      "            \"output_file\":\t{\n",
      "                \"<_io.TextIOWrapper name='spinupPpo/progress.txt' mode='w' encoding='UTF-8'>\":\t{\n",
      "                    \"mode\":\t\"w\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"logger_kwargs\":\t{\n",
      "        \"exp_name\":\t\"experiment\",\n",
      "        \"output_dir\":\t\"spinupPpo\"\n",
      "    },\n",
      "    \"max_ep_len\":\t1000,\n",
      "    \"pi_lr\":\t0.0003,\n",
      "    \"save_freq\":\t10,\n",
      "    \"seed\":\t0,\n",
      "    \"steps_per_epoch\":\t5000,\n",
      "    \"target_kl\":\t0.01,\n",
      "    \"train_pi_iters\":\t80,\n",
      "    \"train_v_iters\":\t80,\n",
      "    \"vf_lr\":\t0.001\n",
      "}\n",
      "WARNING:tensorflow:From /Users/raphacosta/Desktop/Insper7/machineLearning/projeto/spinningup/spinup/algos/ppo/core.py:31: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/raphacosta/Desktop/Insper7/machineLearning/projeto/spinningup/spinup/algos/ppo/core.py:71: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "\u001b[32;1m\n",
      "Number of parameters: \t pi: 4610, \t v: 4545\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/raphacosta/Desktop/Insper7/machineLearning/projeto/spinningup/spinup/utils/mpi_tf.py:63: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "Warning: trajectory cut off by epoch at 9 steps.\n",
      "WARNING:tensorflow:From /Users/raphacosta/Desktop/Insper7/machineLearning/projeto/spinningup/spinup/utils/logx.py:226: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: spinupPpo/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |               0 |\n",
      "|      AverageEpRet |            22.9 |\n",
      "|          StdEpRet |            11.5 |\n",
      "|          MaxEpRet |              80 |\n",
      "|          MinEpRet |               9 |\n",
      "|             EpLen |            22.9 |\n",
      "|      AverageVVals |          0.0272 |\n",
      "|          StdVVals |           0.047 |\n",
      "|          MaxVVals |           0.203 |\n",
      "|          MinVVals |          -0.298 |\n",
      "| TotalEnvInteracts |           5e+03 |\n",
      "|            LossPi |       -8.01e-09 |\n",
      "|             LossV |             283 |\n",
      "|       DeltaLossPi |         -0.0248 |\n",
      "|        DeltaLossV |            -125 |\n",
      "|           Entropy |            0.69 |\n",
      "|                KL |          0.0133 |\n",
      "|          ClipFrac |           0.099 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            2.96 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 8 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |               1 |\n",
      "|      AverageEpRet |            33.1 |\n",
      "|          StdEpRet |              19 |\n",
      "|          MaxEpRet |             107 |\n",
      "|          MinEpRet |              10 |\n",
      "|             EpLen |            33.1 |\n",
      "|      AverageVVals |             7.3 |\n",
      "|          StdVVals |            2.35 |\n",
      "|          MaxVVals |            25.9 |\n",
      "|          MinVVals |            4.78 |\n",
      "| TotalEnvInteracts |           1e+04 |\n",
      "|            LossPi |       -5.91e-08 |\n",
      "|             LossV |             355 |\n",
      "|       DeltaLossPi |         -0.0202 |\n",
      "|        DeltaLossV |            -175 |\n",
      "|           Entropy |           0.677 |\n",
      "|                KL |         0.00983 |\n",
      "|          ClipFrac |          0.0698 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            5.38 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 122 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |               2 |\n",
      "|      AverageEpRet |            56.1 |\n",
      "|          StdEpRet |            33.9 |\n",
      "|          MaxEpRet |             146 |\n",
      "|          MinEpRet |              12 |\n",
      "|             EpLen |            56.1 |\n",
      "|      AverageVVals |            19.6 |\n",
      "|          StdVVals |            2.68 |\n",
      "|          MaxVVals |            37.3 |\n",
      "|          MinVVals |            13.7 |\n",
      "| TotalEnvInteracts |         1.5e+04 |\n",
      "|            LossPi |        -3.8e-08 |\n",
      "|             LossV |             508 |\n",
      "|       DeltaLossPi |          -0.023 |\n",
      "|        DeltaLossV |            -163 |\n",
      "|           Entropy |           0.659 |\n",
      "|                KL |          0.0101 |\n",
      "|          ClipFrac |           0.124 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            7.68 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 94 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |               3 |\n",
      "|      AverageEpRet |            90.9 |\n",
      "|          StdEpRet |            42.8 |\n",
      "|          MaxEpRet |             195 |\n",
      "|          MinEpRet |              12 |\n",
      "|             EpLen |            90.9 |\n",
      "|      AverageVVals |              31 |\n",
      "|          StdVVals |            2.13 |\n",
      "|          MaxVVals |              45 |\n",
      "|          MinVVals |            22.8 |\n",
      "| TotalEnvInteracts |           2e+04 |\n",
      "|            LossPi |       -8.11e-09 |\n",
      "|             LossV |             524 |\n",
      "|       DeltaLossPi |         -0.0146 |\n",
      "|        DeltaLossV |            -131 |\n",
      "|           Entropy |           0.635 |\n",
      "|                KL |         0.00782 |\n",
      "|          ClipFrac |          0.0622 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            10.1 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 37 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |               4 |\n",
      "|      AverageEpRet |             101 |\n",
      "|          StdEpRet |            43.8 |\n",
      "|          MaxEpRet |             177 |\n",
      "|          MinEpRet |              19 |\n",
      "|             EpLen |             101 |\n",
      "|      AverageVVals |            39.8 |\n",
      "|          StdVVals |            3.93 |\n",
      "|          MaxVVals |            46.5 |\n",
      "|          MinVVals |            20.5 |\n",
      "| TotalEnvInteracts |         2.5e+04 |\n",
      "|            LossPi |         8.3e-09 |\n",
      "|             LossV |             419 |\n",
      "|       DeltaLossPi |         -0.0154 |\n",
      "|        DeltaLossV |           -54.5 |\n",
      "|           Entropy |           0.624 |\n",
      "|                KL |         0.00928 |\n",
      "|          ClipFrac |          0.0822 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            12.4 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 93 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |               5 |\n",
      "|      AverageEpRet |             149 |\n",
      "|          StdEpRet |            50.9 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              23 |\n",
      "|             EpLen |             149 |\n",
      "|      AverageVVals |            42.4 |\n",
      "|          StdVVals |            7.02 |\n",
      "|          MaxVVals |            50.9 |\n",
      "|          MinVVals |            8.35 |\n",
      "| TotalEnvInteracts |           3e+04 |\n",
      "|            LossPi |       -5.72e-10 |\n",
      "|             LossV |             497 |\n",
      "|       DeltaLossPi |         -0.0113 |\n",
      "|        DeltaLossV |            -199 |\n",
      "|           Entropy |           0.604 |\n",
      "|                KL |         0.00965 |\n",
      "|          ClipFrac |          0.0792 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            14.6 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 48 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |               6 |\n",
      "|      AverageEpRet |             150 |\n",
      "|          StdEpRet |            40.7 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              49 |\n",
      "|             EpLen |             150 |\n",
      "|      AverageVVals |            48.7 |\n",
      "|          StdVVals |              16 |\n",
      "|          MaxVVals |            66.7 |\n",
      "|          MinVVals |            5.21 |\n",
      "| TotalEnvInteracts |         3.5e+04 |\n",
      "|            LossPi |        6.29e-09 |\n",
      "|             LossV |             230 |\n",
      "|       DeltaLossPi |        -0.00952 |\n",
      "|        DeltaLossV |           -50.3 |\n",
      "|           Entropy |           0.593 |\n",
      "|                KL |         0.00776 |\n",
      "|          ClipFrac |          0.0584 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            16.9 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 192 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |               7 |\n",
      "|      AverageEpRet |             185 |\n",
      "|          StdEpRet |            27.4 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              88 |\n",
      "|             EpLen |             185 |\n",
      "|      AverageVVals |            55.1 |\n",
      "|          StdVVals |            17.1 |\n",
      "|          MaxVVals |            79.5 |\n",
      "|          MinVVals |            4.76 |\n",
      "| TotalEnvInteracts |           4e+04 |\n",
      "|            LossPi |       -1.31e-08 |\n",
      "|             LossV |             319 |\n",
      "|       DeltaLossPi |        -0.00798 |\n",
      "|        DeltaLossV |           -56.2 |\n",
      "|           Entropy |           0.588 |\n",
      "|                KL |         0.00869 |\n",
      "|          ClipFrac |          0.0904 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            19.2 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |               8 |\n",
      "|      AverageEpRet |             172 |\n",
      "|          StdEpRet |            38.3 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              23 |\n",
      "|             EpLen |             172 |\n",
      "|      AverageVVals |              53 |\n",
      "|          StdVVals |            20.7 |\n",
      "|          MaxVVals |            76.5 |\n",
      "|          MinVVals |           -8.03 |\n",
      "| TotalEnvInteracts |         4.5e+04 |\n",
      "|            LossPi |       -4.96e-09 |\n",
      "|             LossV |             190 |\n",
      "|       DeltaLossPi |        -0.00715 |\n",
      "|        DeltaLossV |           -16.7 |\n",
      "|           Entropy |           0.587 |\n",
      "|                KL |         0.00384 |\n",
      "|          ClipFrac |          0.0518 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            21.5 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 33 steps.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: spinupPpo/simple_save/saved_model.pb\n",
      "---------------------------------------\n",
      "|             Epoch |               9 |\n",
      "|      AverageEpRet |             191 |\n",
      "|          StdEpRet |              22 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |             111 |\n",
      "|             EpLen |             191 |\n",
      "|      AverageVVals |              63 |\n",
      "|          StdVVals |              12 |\n",
      "|          MaxVVals |            77.6 |\n",
      "|          MinVVals |            1.97 |\n",
      "| TotalEnvInteracts |           5e+04 |\n",
      "|            LossPi |         4.2e-09 |\n",
      "|             LossV |             416 |\n",
      "|       DeltaLossPi |        -0.00565 |\n",
      "|        DeltaLossV |           -73.1 |\n",
      "|           Entropy |           0.575 |\n",
      "|                KL |         0.00297 |\n",
      "|          ClipFrac |          0.0322 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |              24 |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#env_fn = lambda : TicTacToeEnv()\n",
    "\n",
    "ac_kwargs = dict(hidden_sizes=[64,64], activation=tf.nn.relu)\n",
    "\n",
    "logger_kwargs = dict(output_dir='spinupPpo', exp_name='experiment')\n",
    "\n",
    "ppo(env_fn=env_fn, ac_kwargs=ac_kwargs, steps_per_epoch=5000, epochs=10, logger_kwargs=logger_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./spinupPpo/simple_save/variables/variables\n",
      "Using default action op.\n"
     ]
    }
   ],
   "source": [
    "from spinup.utils.test_policy import load_policy, run_policy\n",
    "_, get_action = load_policy('./spinupPpo')\n",
    "# env = env_fn.make()\n",
    "# env = TicTacToeEnv()\n",
    "env = env_fn()\n",
    "#run_policy(env, get_action, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = env.reset()\n",
    "env.render()\n",
    "num_episodes = 10\n",
    "d = False\n",
    "while not d:\n",
    "    a = get_action(o)\n",
    "    o, r, d, _ = env.step(a)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeEnv()\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "memory = SequentialMemory(limit=5000000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "log_interval = 10000\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    memory=memory,\n",
    "    nb_steps_warmup=1000,\n",
    "    enable_dueling_network=False,\n",
    "    target_model_update=1e-2,\n",
    "    policy=policy\n",
    ")\n",
    "\n",
    "dqn.compile(Adam(lr=1e-5), metrics=['accuracy', 'mae'])\n",
    "\n",
    "# dqn.fit(env, nb_steps=50000, visualize=False, verbose=1,\n",
    "#     callbacks=[ModelIntervalCheckpoint(interval=log_interval)],\n",
    "#     log_interval=log_interval\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_state = np.array([\n",
    "            [1, 0, 2],\n",
    "            [0, 1, 0],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "predict(board_state, 'saved-weights/-0.141.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
